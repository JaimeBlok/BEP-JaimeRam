{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploit Database Analysis: Open Source Exploits and KEV Comparison - BEP Jaime Ram - 5558581\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "This analysis is organized into 6 main steps:\n",
    "\n",
    "1. **Step 1: Data Extraction** - Download and filter ExploitDB data (2020-present)\n",
    "2. **Step 2: OSV Enrichment** - Enrich exploits with OSV vulnerability data\n",
    "3. **Step 3: KEV Splitting** - Split exploits into KEV vs non-KEV groups\n",
    "4. **Step 4: Similarity Analysis** - Semantic similarity analysis of exploit code\n",
    "5. **Step 5: Metadata Analysis** - Feature analysis and predictors\n",
    "6. **Step 6: Visualizations** - Final visualizations for thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Configuration\n",
    "\n",
    "### Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (5.2.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.16.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jaimeram/Library/Python/3.12/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests pandas numpy matplotlib seaborn scikit-learn sentence-transformers scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# Configure scientific plotting style for publication-quality figures\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set scientific publication style\n",
    "plt.style.use('seaborn-v0_8-paper')  # Clean, minimal style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "# Configure matplotlib for publication quality\n",
    "matplotlib.rcParams.update({\n",
    "    # Font settings\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'Times', 'DejaVu Serif'],\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.titlesize': 14,\n",
    "    \n",
    "    # Figure settings\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1,\n",
    "    \n",
    "    # Line and marker settings\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.markersize': 6,\n",
    "    'axes.linewidth': 1,\n",
    "    'grid.linewidth': 0.5,\n",
    "    'grid.alpha': 0.3,\n",
    "    \n",
    "    # Color settings for black & white printing\n",
    "    'axes.prop_cycle': matplotlib.cycler('color', \n",
    "        ['#000000', '#404040', '#808080', '#C0C0C0', '#606060', '#A0A0A0']),\n",
    "    \n",
    "    # Text settings\n",
    "    'text.usetex': False,  # Set to True if you have LaTeX installed\n",
    "    'mathtext.fontset': 'stix',  # Scientific math font\n",
    "    \n",
    "    # Spacing\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Define black & white friendly colormaps\n",
    "# Grayscale colormap\n",
    "cmap_gray = matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "    'grayscale', ['#FFFFFF', '#808080', '#000000'], N=256\n",
    ")\n",
    "\n",
    "# Pattern-based colormap for differentiation (works in B&W)\n",
    "cmap_patterns = ['#000000', '#404040', '#808080', '#C0C0C0', '#606060', '#A0A0A0']\n",
    "\n",
    "# Hatched patterns for bar charts (B&W friendly)\n",
    "hatch_patterns = ['', '///', 'xxx', '...', '+++', '---', '|||']\n",
    "\n",
    "print(\"Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientific plotting helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for scientific plotting\n",
    "def apply_scientific_style(ax, title=None, xlabel=None, ylabel=None):\n",
    "    \"\"\"Apply scientific publication style to an axis\"\"\"\n",
    "    if title:\n",
    "        ax.set_title(title, fontweight='bold', pad=10)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Make left and bottom spines thicker\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "    \n",
    "    # Grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def get_bw_colors(n):\n",
    "    \"\"\"Get n black & white friendly colors\"\"\"\n",
    "    if n <= 6:\n",
    "        return ['#000000', '#404040', '#808080', '#C0C0C0', '#606060', '#A0A0A0'][:n]\n",
    "    else:\n",
    "        # Generate grayscale gradient\n",
    "        return [f'#{int(255 * (1 - i/(n-1))):02x}{int(255 * (1 - i/(n-1))):02x}{int(255 * (1 - i/(n-1))):02x}' \n",
    "                for i in range(n)]\n",
    "\n",
    "def get_bw_hatches(n):\n",
    "    \"\"\"Get n different hatch patterns for B&W differentiation\"\"\"\n",
    "    patterns = ['', '///', 'xxx', '...', '+++', '---', '|||', '\\\\\\\\\\\\', '***', 'ooo']\n",
    "    return patterns[:n]\n",
    "\n",
    "def scientific_bar(ax, x, y, colors=None, hatches=None, **kwargs):\n",
    "    \"\"\"Create a scientific bar chart with B&W friendly styling\"\"\"\n",
    "    if colors is None:\n",
    "        colors = get_bw_colors(len(x) if hasattr(x, '__len__') else 1)\n",
    "    if hatches is None and len(colors) > 1:\n",
    "        hatches = get_bw_hatches(len(colors))\n",
    "    \n",
    "    bars = ax.bar(x, y, color=colors, edgecolor='black', linewidth=0.8, **kwargs)\n",
    "    \n",
    "    if hatches:\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i < len(hatches):\n",
    "                bar.set_hatch(hatches[i])\n",
    "    \n",
    "    return bars\n",
    "\n",
    "def scientific_barh(ax, y, x, colors=None, hatches=None, **kwargs):\n",
    "    \"\"\"Create a scientific horizontal bar chart with B&W friendly styling\"\"\"\n",
    "    if colors is None:\n",
    "        colors = get_bw_colors(len(y) if hasattr(y, '__len__') else 1)\n",
    "    if hatches is None and len(colors) > 1:\n",
    "        hatches = get_bw_hatches(len(colors))\n",
    "    \n",
    "    bars = ax.barh(y, x, color=colors, edgecolor='black', linewidth=0.8, **kwargs)\n",
    "    \n",
    "    if hatches:\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i < len(hatches):\n",
    "                bar.set_hatch(hatches[i])\n",
    "    \n",
    "    return bars\n",
    "\n",
    "def scientific_heatmap(ax, data, **kwargs):\n",
    "    \"\"\"Create a scientific heatmap with grayscale colormap\"\"\"\n",
    "    # Use grayscale colormap for B&W printing\n",
    "    cmap = kwargs.pop('cmap', cmap_gray)\n",
    "    return sns.heatmap(data, ax=ax, cmap=cmap, cbar_kws={'label': 'Value'}, \n",
    "                      linewidths=0.5, linecolor='black', **kwargs)\n",
    "\n",
    "print(\"Scientific plotting helper functions loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific Plotting Style\n",
    "\n",
    "All visualizations in this notebook use a scientific publication style that is:\n",
    "- **Black & white friendly**: Uses grayscale colors and hatch patterns\n",
    "- **High resolution**: 300 DPI for publication quality\n",
    "- **Consistent formatting**: Times New Roman font, proper spacing\n",
    "- **Professional appearance**: Clean, minimal design suitable for academic papers\n",
    "\n",
    "### Usage in plots:\n",
    "- Use `apply_scientific_style(ax)` to format axes\n",
    "- Use `scientific_bar()` or `scientific_barh()` for bar charts\n",
    "- Use `scientific_heatmap()` for heatmaps\n",
    "- Use `get_bw_colors(n)` for grayscale color schemes\n",
    "- Use `get_bw_hatches(n)` for hatch patterns\n",
    "\n",
    "All plots are automatically saved at 300 DPI and are suitable for inclusion in academic papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output directories created:\n",
      "  step1: outputs/step1_data_extraction\n",
      "  step2: outputs/step2_osv_enrichment\n",
      "  step3: outputs/step3_kev_splitting\n",
      "  step4: outputs/step4_similarity_analysis\n",
      "  step5: outputs/step5_metadata_analysis\n",
      "  step6: outputs/step6_visualizations\n"
     ]
    }
   ],
   "source": [
    "# Setup: Create output directories and configure paths\n",
    "import os\n",
    "\n",
    "# Create output directories for each step\n",
    "output_dirs = {\n",
    "    'step1': 'outputs/step1_data_extraction',\n",
    "    'step2': 'outputs/step2_osv_enrichment',\n",
    "    'step3': 'outputs/step3_kev_splitting',\n",
    "    'step4': 'outputs/step4_similarity_analysis',\n",
    "    'step5': 'outputs/step5_metadata_analysis',\n",
    "    'step6': 'outputs/step6_visualizations'\n",
    "}\n",
    "\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"✓ Output directories created:\")\n",
    "for step, path in output_dirs.items():\n",
    "    print(f\"  {step}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Download completed! Saved to: outputs/step1_data_extraction/files_exploits.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Download ExploitDB CSV\n",
    "def download_exploitdb_csv():\n",
    "    \"\"\"Download ExploitDB CSV file\"\"\"\n",
    "    url = \"https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv\"\n",
    "    output_path = os.path.join(output_dirs['step1'], 'files_exploits.csv')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✓ Download completed! Saved to: {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(f\"✗ Error downloading: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Download the CSV\n",
    "exploitdb_path = download_exploitdb_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "### 1.1 Download ExploitDB Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPLOITDB DATASET EXPLORATION\n",
      "============================================================\n",
      "\n",
      "Dataset Shape: 46,947 rows × 17 columns\n",
      "\n",
      "Variables (Columns):\n",
      "------------------------------------------------------------\n",
      " 1. id                   - 46,947 non-null (100.0%)\n",
      " 2. file                 - 46,947 non-null (100.0%)\n",
      " 3. description          - 46,947 non-null (100.0%)\n",
      " 4. date_published       - 46,947 non-null (100.0%)\n",
      " 5. author               - 46,947 non-null (100.0%)\n",
      " 6. type                 - 46,947 non-null (100.0%)\n",
      " 7. platform             - 46,947 non-null (100.0%)\n",
      " 8. port                 - 3,355 non-null (7.1%)\n",
      " 9. date_added           - 46,947 non-null (100.0%)\n",
      "10. date_updated         - 39,853 non-null (84.9%)\n",
      "11. verified             - 46,947 non-null (100.0%)\n",
      "12. codes                - 32,349 non-null (68.9%)\n",
      "13. tags                 - 5,159 non-null (11.0%)\n",
      "14. aliases              - 484 non-null (1.0%)\n",
      "15. screenshot_url       - 1,458 non-null (3.1%)\n",
      "16. application_url      - 8,335 non-null (17.8%)\n",
      "17. source_url           - 19,522 non-null (41.6%)\n",
      "\n",
      "First 5 rows:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>description</th>\n",
       "      <th>date_published</th>\n",
       "      <th>author</th>\n",
       "      <th>type</th>\n",
       "      <th>platform</th>\n",
       "      <th>port</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>verified</th>\n",
       "      <th>codes</th>\n",
       "      <th>tags</th>\n",
       "      <th>aliases</th>\n",
       "      <th>screenshot_url</th>\n",
       "      <th>application_url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16929</td>\n",
       "      <td>exploits/aix/dos/16929.rb</td>\n",
       "      <td>AIX Calendar Manager Service Daemon (rpc.cmsd)...</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>Metasploit</td>\n",
       "      <td>dos</td>\n",
       "      <td>aix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>2011-03-06</td>\n",
       "      <td>1</td>\n",
       "      <td>CVE-2009-3699;OSVDB-58726</td>\n",
       "      <td>Metasploit Framework (MSF)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://aix.software.ibm.com/aix/efixes/securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19046</td>\n",
       "      <td>exploits/aix/dos/19046.txt</td>\n",
       "      <td>AppleShare IP Mail Server 5.0.3 - Buffer Overflow</td>\n",
       "      <td>1999-10-15</td>\n",
       "      <td>Chris Wedgwood</td>\n",
       "      <td>dos</td>\n",
       "      <td>aix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1999-10-15</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>CVE-1999-1015;OSVDB-5970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.securityfocus.com/bid/61/info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19049</td>\n",
       "      <td>exploits/aix/dos/19049.txt</td>\n",
       "      <td>BSDI 4.0 tcpmux / inetd - Crash</td>\n",
       "      <td>1998-04-07</td>\n",
       "      <td>Mark Schaefer</td>\n",
       "      <td>dos</td>\n",
       "      <td>aix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1998-04-07</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>OSVDB-82889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.securityfocus.com/bid/66/info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33943</td>\n",
       "      <td>exploits/aix/dos/33943.txt</td>\n",
       "      <td>Flussonic Media Server 4.1.25 &lt; 4.3.3 - Arbitr...</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>BGA Security</td>\n",
       "      <td>dos</td>\n",
       "      <td>aix</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>OSVDB-108610;OSVDB-108609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19418</td>\n",
       "      <td>exploits/aix/dos/19418.txt</td>\n",
       "      <td>IBM AIX 4.3.1 - 'adb' Denial of Service</td>\n",
       "      <td>1999-07-12</td>\n",
       "      <td>GZ Apple</td>\n",
       "      <td>dos</td>\n",
       "      <td>aix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1999-07-12</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>1</td>\n",
       "      <td>OSVDB-83455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.securityfocus.com/bid/520/info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        file  \\\n",
       "0  16929   exploits/aix/dos/16929.rb   \n",
       "1  19046  exploits/aix/dos/19046.txt   \n",
       "2  19049  exploits/aix/dos/19049.txt   \n",
       "3  33943  exploits/aix/dos/33943.txt   \n",
       "4  19418  exploits/aix/dos/19418.txt   \n",
       "\n",
       "                                         description date_published  \\\n",
       "0  AIX Calendar Manager Service Daemon (rpc.cmsd)...     2010-11-11   \n",
       "1  AppleShare IP Mail Server 5.0.3 - Buffer Overflow     1999-10-15   \n",
       "2                    BSDI 4.0 tcpmux / inetd - Crash     1998-04-07   \n",
       "3  Flussonic Media Server 4.1.25 < 4.3.3 - Arbitr...     2014-07-01   \n",
       "4            IBM AIX 4.3.1 - 'adb' Denial of Service     1999-07-12   \n",
       "\n",
       "           author type platform    port  date_added date_updated  verified  \\\n",
       "0      Metasploit  dos      aix     NaN  2010-11-11   2011-03-06         1   \n",
       "1  Chris Wedgwood  dos      aix     NaN  1999-10-15   2014-01-02         1   \n",
       "2   Mark Schaefer  dos      aix     NaN  1998-04-07   2014-01-02         1   \n",
       "3    BGA Security  dos      aix  8080.0  2014-07-01   2014-07-01         0   \n",
       "4        GZ Apple  dos      aix     NaN  1999-07-12   2017-11-15         1   \n",
       "\n",
       "                       codes                        tags aliases  \\\n",
       "0  CVE-2009-3699;OSVDB-58726  Metasploit Framework (MSF)     NaN   \n",
       "1   CVE-1999-1015;OSVDB-5970                         NaN     NaN   \n",
       "2                OSVDB-82889                         NaN     NaN   \n",
       "3  OSVDB-108610;OSVDB-108609                         NaN     NaN   \n",
       "4                OSVDB-83455                         NaN     NaN   \n",
       "\n",
       "  screenshot_url application_url  \\\n",
       "0            NaN             NaN   \n",
       "1            NaN             NaN   \n",
       "2            NaN             NaN   \n",
       "3            NaN             NaN   \n",
       "4            NaN             NaN   \n",
       "\n",
       "                                          source_url  \n",
       "0  http://aix.software.ibm.com/aix/efixes/securit...  \n",
       "1          https://www.securityfocus.com/bid/61/info  \n",
       "2          https://www.securityfocus.com/bid/66/info  \n",
       "3                                                NaN  \n",
       "4         https://www.securityfocus.com/bid/520/info  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset exploration saved to: outputs/step1_data_extraction/\n"
     ]
    }
   ],
   "source": [
    "# Load and explore ExploitDB dataset\n",
    "if exploitdb_path:\n",
    "    df_exploitdb = pd.read_csv(exploitdb_path, encoding='latin-1')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXPLOITDB DATASET EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nDataset Shape: {df_exploitdb.shape[0]:,} rows × {df_exploitdb.shape[1]} columns\")\n",
    "    \n",
    "    print(f\"\\nVariables (Columns):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, col in enumerate(df_exploitdb.columns, 1):\n",
    "        non_null = df_exploitdb[col].notna().sum()\n",
    "        pct = (non_null / len(df_exploitdb)) * 100\n",
    "        print(f\"{i:2d}. {col:20s} - {non_null:,} non-null ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(\"-\" * 60)\n",
    "    display(df_exploitdb.head(5))\n",
    "    \n",
    "    # Save sample\n",
    "    df_exploitdb.head(5).to_csv(os.path.join(output_dirs['step1'], 'sample_exploitdb.csv'), index=False)\n",
    "    \n",
    "    # Basic statistics (no plots - plots are in Final Visualizations)\n",
    "    \n",
    "    print(f\"\\n✓ Dataset exploration saved to: {output_dirs['step1']}/\")\n",
    "else:\n",
    "    print(\"✗ Cannot explore dataset - download failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Exploration: ExploitDB\n",
    "\n",
    "Explore the ExploitDB dataset structure, variables, and sample data.\n",
    "\n",
    "### 1.3 Filter ExploitDB Data (2020-Present)\n",
    "\n",
    "Filter the ExploitDB data to only include exploits from 2020 onwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['id', 'file', 'description', 'date_published', 'author', 'type', 'platform', 'port', 'date_added', 'date_updated', 'verified', 'codes', 'tags', 'aliases', 'screenshot_url', 'application_url', 'source_url']\n",
      "Exploits from 2020 onwards: 4316\n",
      "\n",
      "Note: We will use OSV data to identify open source exploits\n",
      "      (exploits with CVEs that exist in OSV database)\n",
      "\n",
      "✓ Filtered data saved to: outputs/step1_data_extraction/exploits_2020_present.csv\n",
      "\n",
      "First 5 rows of filtered data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>description</th>\n",
       "      <th>date_published</th>\n",
       "      <th>author</th>\n",
       "      <th>type</th>\n",
       "      <th>platform</th>\n",
       "      <th>port</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>verified</th>\n",
       "      <th>codes</th>\n",
       "      <th>tags</th>\n",
       "      <th>aliases</th>\n",
       "      <th>screenshot_url</th>\n",
       "      <th>application_url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>47921</td>\n",
       "      <td>exploits/android/dos/47921.txt</td>\n",
       "      <td>Android - ashmem Readonly Bypasses via remap_f...</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>Google Security Research</td>\n",
       "      <td>dos</td>\n",
       "      <td>android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>1</td>\n",
       "      <td>CVE-2020-0009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://bugs.chromium.org/p/project-zero/issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>51438</td>\n",
       "      <td>exploits/android/dos/51438.py</td>\n",
       "      <td>FLEX 1080 &lt; 1085 Web 1.6.0 - Denial of Service</td>\n",
       "      <td>2023-05-13</td>\n",
       "      <td>Mr Empy</td>\n",
       "      <td>dos</td>\n",
       "      <td>android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-13</td>\n",
       "      <td>2023-05-13</td>\n",
       "      <td>0</td>\n",
       "      <td>CVE-2022-2591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>47920</td>\n",
       "      <td>exploits/android/dos/47920.txt</td>\n",
       "      <td>WeChat - Memory Corruption in CAudioJBM::Input...</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>Google Security Research</td>\n",
       "      <td>dos</td>\n",
       "      <td>android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://bugs.chromium.org/p/project-zero/issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>48129</td>\n",
       "      <td>exploits/android/local/48129.rb</td>\n",
       "      <td>Android Binder - Use-After-Free (Metasploit)</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>Metasploit</td>\n",
       "      <td>local</td>\n",
       "      <td>android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>1</td>\n",
       "      <td>CVE-2019-2215</td>\n",
       "      <td>Metasploit Framework (MSF)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://raw.githubusercontent.com/rapid7/metas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>48129</td>\n",
       "      <td>exploits/android/local/48129.rb</td>\n",
       "      <td>Android Binder - Use-After-Free (Metasploit)</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>Metasploit</td>\n",
       "      <td>local</td>\n",
       "      <td>android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>1</td>\n",
       "      <td>CVE-2019-2215</td>\n",
       "      <td>Local</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://raw.githubusercontent.com/rapid7/metas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                             file  \\\n",
       "93   47921   exploits/android/dos/47921.txt   \n",
       "114  51438    exploits/android/dos/51438.py   \n",
       "166  47920   exploits/android/dos/47920.txt   \n",
       "177  48129  exploits/android/local/48129.rb   \n",
       "178  48129  exploits/android/local/48129.rb   \n",
       "\n",
       "                                           description date_published  \\\n",
       "93   Android - ashmem Readonly Bypasses via remap_f...     2020-01-14   \n",
       "114     FLEX 1080 < 1085 Web 1.6.0 - Denial of Service     2023-05-13   \n",
       "166  WeChat - Memory Corruption in CAudioJBM::Input...     2020-01-14   \n",
       "177       Android Binder - Use-After-Free (Metasploit)     2020-02-24   \n",
       "178       Android Binder - Use-After-Free (Metasploit)     2020-02-24   \n",
       "\n",
       "                       author   type platform  port  date_added date_updated  \\\n",
       "93   Google Security Research    dos  android   NaN  2020-01-14   2020-01-14   \n",
       "114                   Mr Empy    dos  android   NaN  2023-05-13   2023-05-13   \n",
       "166  Google Security Research    dos  android   NaN  2020-01-14   2020-01-14   \n",
       "177                Metasploit  local  android   NaN  2020-02-24   2020-02-24   \n",
       "178                Metasploit  local  android   NaN  2020-02-24   2020-02-24   \n",
       "\n",
       "     verified          codes                        tags aliases  \\\n",
       "93          1  CVE-2020-0009                         NaN     NaN   \n",
       "114         0  CVE-2022-2591                         NaN     NaN   \n",
       "166         1            NaN                         NaN     NaN   \n",
       "177         1  CVE-2019-2215  Metasploit Framework (MSF)     NaN   \n",
       "178         1  CVE-2019-2215                       Local     NaN   \n",
       "\n",
       "    screenshot_url application_url  \\\n",
       "93             NaN             NaN   \n",
       "114            NaN             NaN   \n",
       "166            NaN             NaN   \n",
       "177            NaN             NaN   \n",
       "178            NaN             NaN   \n",
       "\n",
       "                                            source_url  \n",
       "93   https://bugs.chromium.org/p/project-zero/issue...  \n",
       "114                                                NaN  \n",
       "166  https://bugs.chromium.org/p/project-zero/issue...  \n",
       "177  https://raw.githubusercontent.com/rapid7/metas...  \n",
       "178  https://raw.githubusercontent.com/rapid7/metas...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and filter ExploitDB data - ONLY filter on date, not on open source\n",
    "# We will use OSV data to determine which exploits are open source\n",
    "def load_and_filter_exploitdb():\n",
    "    \"\"\"Load ExploitDB CSV and filter ONLY on 2020-present (no open source filtering yet)\"\"\"\n",
    "    \n",
    "    # Read the CSV from the output directory\n",
    "    csv_path = os.path.join(output_dirs['step1'], 'files_exploits.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"✗ Error: {csv_path} not found. Please run the download cell first.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "    \n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Filter on date (2020-present) ONLY\n",
    "    date_col = None\n",
    "    for col in ['date_published', 'date_added', 'date', 'published_date']:\n",
    "        if col in df.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        df_filtered = df[df[date_col] >= '2020-01-01'].copy()\n",
    "        print(f\"Exploits from 2020 onwards: {len(df_filtered)}\")\n",
    "    else:\n",
    "        df_filtered = df.copy()\n",
    "    \n",
    "    print(f\"\\nNote: We will use OSV data to identify open source exploits\")\n",
    "    print(f\"      (exploits with CVEs that exist in OSV database)\")\n",
    "    \n",
    "    # Save filtered data\n",
    "    filtered_path = os.path.join(output_dirs['step1'], 'exploits_2020_present.csv')\n",
    "    df_filtered.to_csv(filtered_path, index=False)\n",
    "    print(f\"\\n✓ Filtered data saved to: {filtered_path}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "exploitdb_df = load_and_filter_exploitdb()\n",
    "if exploitdb_df is not None:\n",
    "    print(f\"\\nFirst 5 rows of filtered data:\")\n",
    "    display(exploitdb_df.head())\n",
    "else:\n",
    "    print(\"✗ Cannot proceed - data loading failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: OSV Enrichment\n",
    "\n",
    "Enrich exploits with OSV vulnerability data to identify open-source exploits.\n",
    "\n",
    "We will:\n",
    "1. Extract all CVEs from ExploitDB exploits\n",
    "2. Query OSV API for these CVEs\n",
    "3. Filter exploits: only keep exploits where the CVE exists in OSV (these are open source)\n",
    "4. Enrich the filtered exploits with OSV data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inspecting ExploitDB data structure ===\n",
      "Columns: ['id', 'file', 'description', 'date_published', 'author', 'type', 'platform', 'port', 'date_added', 'date_updated', 'verified', 'codes', 'tags', 'aliases', 'screenshot_url', 'application_url', 'source_url']\n",
      "\n",
      "Sample data from potential CVE columns:\n",
      "\n",
      "codes column (first 5 non-null values):\n",
      "  Row 93: CVE-2020-0009\n",
      "  Row 114: CVE-2022-2591\n",
      "  Row 177: CVE-2019-2215\n",
      "  Row 178: CVE-2019-2215\n",
      "  Row 200: CVE-2018-20523\n",
      "\n",
      "aliases column (first 5 non-null values):\n",
      "  Row 10850: KR00K\n",
      "  Row 38173: SMBGhost\n",
      "  Row 41389: SMBGhost\n",
      "\n",
      "==================================================\n",
      "Extracting CVE IDs from ExploitDB data...\n",
      "  Column 'codes': 1514 unique CVEs\n",
      "  Column 'description': 4 unique CVEs\n",
      "\n",
      "✓ Total unique CVEs found across all columns: 1518\n",
      "Sample CVEs: ['CVE-2012-5958', 'CVE-2021-34370', 'CVE-2021-44228', 'CVE-2024-10758', 'CVE-2025-29927', 'CVE-2022-2591', 'CVE-2022-37061', 'CVE-2021-45428', 'CVE-2023-31703', 'CVE-2019-20215']\n",
      "\n",
      "Querying OSV API for 1518 CVEs...\n",
      "This may take a while - OSV API will be queried for each CVE...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "✗ Batch failed (400), using individual queries...\n",
      "\n",
      "Batch query summary: 0 successful, 31 failed\n",
      "\n",
      "✓ Retrieved OSV data for 508 CVEs (33.5% match rate)\n",
      "\n",
      "============================================================\n",
      "OPEN SOURCE IDENTIFICATION:\n",
      "============================================================\n",
      "CVEs NOT in OSV (likely not open source): 1010\n"
     ]
    }
   ],
   "source": [
    "# OSV API endpoint\n",
    "OSV_API_BASE = \"https://api.osv.dev/v1\"\n",
    "\n",
    "def query_osv_by_cve(cve_id):\n",
    "    \"\"\"Query OSV API for a specific CVE ID\"\"\"\n",
    "    url = f\"{OSV_API_BASE}/vulns/{cve_id}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 404:\n",
    "            return None  # CVE not found in OSV\n",
    "        else:\n",
    "            print(f\"Error querying OSV for {cve_id}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception querying OSV for {cve_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_query_osv_by_cves(cve_ids, batch_size=50):\n",
    "    \"\"\"Query OSV API for multiple CVE IDs using batch endpoint for better performance\"\"\"\n",
    "    url = f\"{OSV_API_BASE}/querybatch\"\n",
    "    \n",
    "    all_results = {}\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    # Process in batches using OSV's batch query endpoint\n",
    "    total_batches = (len(cve_ids) - 1) // batch_size + 1\n",
    "    \n",
    "    for i in range(0, len(cve_ids), batch_size):\n",
    "        batch = cve_ids[i:i+batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        \n",
    "        # Prepare batch query - OSV querybatch expects queries in format:\n",
    "        # [{\"id\": \"CVE-2020-1234\"}, {\"id\": \"CVE-2020-5678\"}, ...]\n",
    "        queries = [{\"id\": cve_id} for cve_id in batch]\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json={\"queries\": queries}, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                batch_results = response.json()\n",
    "                \n",
    "                # OSV returns results in format: {\"results\": [{\"vulns\": [...]}, ...]}\n",
    "                batch_matches = 0\n",
    "                if 'results' in batch_results:\n",
    "                    for idx, result in enumerate(batch_results['results']):\n",
    "                        if 'vulns' in result and len(result['vulns']) > 0:\n",
    "                            # Take the first vulnerability (most relevant)\n",
    "                            cve_id = batch[idx]\n",
    "                            all_results[cve_id] = result['vulns'][0]\n",
    "                            batch_matches += 1\n",
    "                else:\n",
    "                    # Fallback: try direct structure\n",
    "                    for idx, cve_id in enumerate(batch):\n",
    "                        if idx < len(batch_results) and batch_results[idx]:\n",
    "                            all_results[cve_id] = batch_results[idx]\n",
    "                            batch_matches += 1\n",
    "                \n",
    "                print(f\"✓ {batch_matches}/{len(batch)} matches\")\n",
    "                successful_batches += 1\n",
    "            else:\n",
    "                # Fallback to individual queries if batch fails\n",
    "                print(f\"✗ Batch failed ({response.status_code}), using individual queries...\")\n",
    "                failed_batches += 1\n",
    "                for cve_id in batch:\n",
    "                    vuln_data = query_osv_by_cve(cve_id)\n",
    "                    if vuln_data:\n",
    "                        all_results[cve_id] = vuln_data\n",
    "        except Exception as e:\n",
    "            # Fallback to individual queries on exception\n",
    "            print(f\"✗ Batch exception: {e}, using individual queries...\")\n",
    "            failed_batches += 1\n",
    "            for cve_id in batch:\n",
    "                vuln_data = query_osv_by_cve(cve_id)\n",
    "                if vuln_data:\n",
    "                    all_results[cve_id] = vuln_data\n",
    "        \n",
    "        # Small delay to be respectful to the API\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    print(f\"\\nBatch query summary: {successful_batches} successful, {failed_batches} failed\")\n",
    "    return all_results\n",
    "\n",
    "# Extract unique CVE IDs from ExploitDB data - check ALL possible columns\n",
    "def extract_cves_from_exploitdb(df):\n",
    "    \"\"\"Extract CVE IDs from ExploitDB dataframe - checks ALL columns that might contain CVEs\"\"\"\n",
    "    # Check ALL possible columns where CVEs might be stored\n",
    "    possible_cve_cols = ['codes', 'aliases', 'cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID', \n",
    "                         'description', 'tags', 'file', 'code']\n",
    "    \n",
    "    # Find which columns exist\n",
    "    available_cols = [col for col in possible_cve_cols if col in df.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return []\n",
    "    \n",
    "    # Extract CVEs from all relevant columns\n",
    "    cves = []\n",
    "    cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "    \n",
    "    for col in available_cols:\n",
    "        col_cves = []\n",
    "        for idx, row in df.iterrows():\n",
    "            value = row[col]\n",
    "            if pd.notna(value) and value != '':\n",
    "                value_str = str(value).upper()\n",
    "                # Try to extract CVE patterns (CVE-YYYY-NNNNN)\n",
    "                found_cves = re.findall(cve_pattern, value_str)\n",
    "                col_cves.extend(found_cves)\n",
    "        \n",
    "        unique_col_cves = len(set(col_cves))\n",
    "        if unique_col_cves > 0:\n",
    "            print(f\"  Column '{col}': {unique_col_cves} unique CVEs\")\n",
    "            cves.extend(col_cves)\n",
    "    \n",
    "    # Remove duplicates and normalize\n",
    "    unique_cves = list(set([cve.upper().strip() for cve in cves if cve and cve.startswith('CVE-')]))\n",
    "    print(f\"\\n✓ Total unique CVEs found across all columns: {len(unique_cves)}\")\n",
    "    return unique_cves\n",
    "\n",
    "# First, let's inspect the data to see where CVEs are stored\n",
    "print(\"=== Inspecting ExploitDB data structure ===\")\n",
    "print(f\"Columns: {exploitdb_df.columns.tolist()}\\n\")\n",
    "\n",
    "# Check a few sample rows to see where CVEs might be\n",
    "print(\"Sample data from potential CVE columns:\")\n",
    "for col in ['codes', 'aliases', 'cve', 'CVE']:\n",
    "    if col in exploitdb_df.columns:\n",
    "        sample = exploitdb_df[col].dropna().head(5)\n",
    "        print(f\"\\n{col} column (first 5 non-null values):\")\n",
    "        for idx, val in sample.items():\n",
    "            print(f\"  Row {idx}: {val}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Extracting CVE IDs from ExploitDB data...\")\n",
    "unique_cves = extract_cves_from_exploitdb(exploitdb_df)\n",
    "if len(unique_cves) > 0:\n",
    "    print(f\"Sample CVEs: {unique_cves[:10]}\")\n",
    "\n",
    "# Query OSV API for these CVEs\n",
    "if len(unique_cves) > 0:\n",
    "    print(f\"\\nQuerying OSV API for {len(unique_cves)} CVEs...\")\n",
    "    print(f\"This may take a while - OSV API will be queried for each CVE...\")\n",
    "    \n",
    "    # Try batch first, but if it fails, use individual queries\n",
    "    osv_data = batch_query_osv_by_cves(unique_cves, batch_size=50)\n",
    "    \n",
    "    # If batch didn't work well, use individual queries\n",
    "    if len(osv_data) < len(unique_cves) * 0.1:  # If less than 10% match, batch probably failed\n",
    "        print(f\"\\nBatch queries didn't work well ({len(osv_data)}/{len(unique_cves)} matches)\")\n",
    "        print(f\"Using individual queries for better reliability...\")\n",
    "        osv_data = {}\n",
    "        for i, cve_id in enumerate(unique_cves):\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"  Progress: {i+1}/{len(unique_cves)} CVEs queried ({len(osv_data)} matches so far)...\")\n",
    "            vuln_data = query_osv_by_cve(cve_id)\n",
    "            if vuln_data:\n",
    "                osv_data[cve_id] = vuln_data\n",
    "            time.sleep(0.1)  # Small delay\n",
    "    \n",
    "    print(f\"\\n✓ Retrieved OSV data for {len(osv_data)} CVEs ({len(osv_data)/len(unique_cves)*100:.1f}% match rate)\")\n",
    "    \n",
    "    # Get set of CVEs that exist in OSV (these are open source)\n",
    "    osv_cve_set = set(osv_data.keys())\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OPEN SOURCE IDENTIFICATION:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"CVEs NOT in OSV (likely not open source): {len(unique_cves) - len(osv_cve_set)}\")\n",
    "else:\n",
    "    osv_data = {}\n",
    "    osv_cve_set = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IDENTIFYING OPEN SOURCE EXPLOITS (HYBRID APPROACH)\n",
      "============================================================\n",
      "Extracting CVEs from 9 columns...\n",
      "\n",
      "Exploits with CVE: 1535\n",
      "Exploits without CVE: 2781\n",
      "\n",
      "Method 1 - Exploits with CVE in OSV: 520\n",
      "  Exploits without CVE but with open source indicators: 1584\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS:\n",
      "============================================================\n",
      "✓ Open source exploits (total): 2104\n",
      "✗ Non-open source exploits: 2212\n",
      "\n",
      "Percentage breakdown:\n",
      "  - Open source: 48.7%\n",
      "  - Non-open source: 51.3%\n",
      "\n",
      "CVE statistics:\n",
      "  - Exploits with CVE: 1535 (35.6%)\n",
      "  - Exploits with CVE in OSV: 520 (33.9% of exploits with CVE)\n",
      "Using column 'codes' for CVE extraction\n",
      "\n",
      "Enriched dataset: 2104 exploits\n",
      "✓ With OSV match: 520 exploits (24.7%)\n",
      "  - With severity: 514\n",
      "  - With summary: 70\n",
      "\n",
      "============================================================\n",
      "FINAL OPEN SOURCE EXPLOITS DATASET:\n",
      "============================================================\n",
      "Shape: (2104, 23)\n",
      "OSV columns: ['osv_id', 'osv_summary', 'osv_severity', 'osv_database_specific', 'osv_affected', 'osv_references']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/t6wnqflx2mz5tzgwpxn52wpc0000gn/T/ipykernel_64302/2940503883.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exploits_without_cve['has_open_source_indicators'] = False\n",
      "/var/folders/36/t6wnqflx2mz5tzgwpxn52wpc0000gn/T/ipykernel_64302/2940503883.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exploits_without_cve['has_open_source_indicators'] = exploits_without_cve['has_open_source_indicators'] | \\\n"
     ]
    }
   ],
   "source": [
    "def filter_and_enrich_with_osv(exploit_df, osv_data_dict, osv_cve_set):\n",
    "    \"\"\"Filter exploits to identify open source using HYBRID approach:\n",
    "    1. Exploits with CVE in OSV = open source\n",
    "    2. Exploits without CVE but with open source indicators = also open source\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"IDENTIFYING OPEN SOURCE EXPLOITS (HYBRID APPROACH)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract CVEs from each exploit\n",
    "    def extract_cve_from_value(value):\n",
    "        \"\"\"Extract CVE ID from a value (handles strings with multiple CVEs)\"\"\"\n",
    "        if pd.isna(value) or value == '':\n",
    "            return []\n",
    "        value_str = str(value).upper()\n",
    "        cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "        matches = re.findall(cve_pattern, value_str)\n",
    "        return matches\n",
    "    \n",
    "    # Check ALL possible CVE columns\n",
    "    cve_cols_to_check = ['codes', 'aliases', 'cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID', 'description', 'tags']\n",
    "    \n",
    "    # Initialize column with empty lists for each row\n",
    "    exploit_df['extracted_cves'] = exploit_df.apply(lambda x: [], axis=1)\n",
    "    \n",
    "    # Extract CVEs from ALL columns that might contain them\n",
    "    print(f\"Extracting CVEs from {len(cve_cols_to_check)} columns...\")\n",
    "    for idx, row in exploit_df.iterrows():\n",
    "        all_cves = []\n",
    "        for col in cve_cols_to_check:\n",
    "            if col in exploit_df.columns:\n",
    "                value = row[col]\n",
    "                cves = extract_cve_from_value(value)\n",
    "                all_cves.extend(cves)\n",
    "        exploit_df.at[idx, 'extracted_cves'] = list(set(all_cves))  # Remove duplicates\n",
    "    \n",
    "    # Count exploits with/without CVEs\n",
    "    exploits_with_cve = exploit_df[exploit_df['extracted_cves'].apply(lambda x: len(x) > 0)]\n",
    "    exploits_without_cve = exploit_df[exploit_df['extracted_cves'].apply(lambda x: len(x) == 0)]\n",
    "    \n",
    "    print(f\"\\nExploits with CVE: {len(exploits_with_cve)}\")\n",
    "    print(f\"Exploits without CVE: {len(exploits_without_cve)}\")\n",
    "    \n",
    "    # Method 1: Exploits with CVE that's in OSV = open source\n",
    "    if osv_cve_set:\n",
    "        exploit_df['has_osv_cve'] = exploit_df['extracted_cves'].apply(\n",
    "            lambda cves: any(cve in osv_cve_set for cve in cves) if cves else False\n",
    "        )\n",
    "        osv_cve_matches = exploit_df[exploit_df['has_osv_cve']]\n",
    "        print(f\"\\nMethod 1 - Exploits with CVE in OSV: {len(osv_cve_matches)}\")\n",
    "    else:\n",
    "        exploit_df['has_osv_cve'] = False\n",
    "        osv_cve_matches = pd.DataFrame()\n",
    "        print(f\"\\nMethod 1 - No OSV data available\")\n",
    "    \n",
    "    # Method 2: Exploits without CVE but with open source indicators\n",
    "    \n",
    "    # Open source keywords (for exploits without CVE)\n",
    "    open_source_keywords = [\n",
    "        'php', 'python', 'ruby', 'perl', 'java', 'javascript', 'typescript',\n",
    "        'node', 'nodejs', 'go', 'golang', 'rust', 'django', 'flask', 'rails',\n",
    "        'laravel', 'spring', 'express', 'react', 'angular', 'vue', 'npm',\n",
    "        'pip', 'composer', 'gem', 'cargo', 'github', 'gitlab', 'bitbucket'\n",
    "    ]\n",
    "    pattern = '|'.join([re.escape(kw) for kw in open_source_keywords])\n",
    "    \n",
    "    # Check exploits without CVE\n",
    "    exploits_without_cve['has_open_source_indicators'] = False\n",
    "    \n",
    "    for col in ['platform', 'description', 'file', 'type']:\n",
    "        if col in exploits_without_cve.columns:\n",
    "            exploits_without_cve['has_open_source_indicators'] = exploits_without_cve['has_open_source_indicators'] | \\\n",
    "                exploits_without_cve[col].str.lower().str.contains(pattern, na=False, regex=True)\n",
    "    \n",
    "    open_source_by_indicators = exploits_without_cve[exploits_without_cve['has_open_source_indicators']]\n",
    "    print(f\"  Exploits without CVE but with open source indicators: {len(open_source_by_indicators)}\")\n",
    "    \n",
    "    # Combine both methods\n",
    "    exploit_df['is_open_source'] = exploit_df['has_osv_cve'].copy()\n",
    "    \n",
    "    # Add exploits without CVE but with indicators\n",
    "    if len(open_source_by_indicators) > 0:\n",
    "        exploit_df.loc[open_source_by_indicators.index, 'is_open_source'] = True\n",
    "    \n",
    "    # Final open source exploits\n",
    "    open_source_exploits = exploit_df[exploit_df['is_open_source']].copy()\n",
    "    \n",
    "    # Statistics\n",
    "    total_with_cve_in_osv = len(osv_cve_matches) if osv_cve_set else 0\n",
    "    total_without_cve_but_indicators = len(open_source_by_indicators)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Open source exploits (total): {len(open_source_exploits)}\")\n",
    "    print(f\"✗ Non-open source exploits: {len(exploit_df) - len(open_source_exploits)}\")\n",
    "    print(f\"\\nPercentage breakdown:\")\n",
    "    print(f\"  - Open source: {len(open_source_exploits)/len(exploit_df)*100:.1f}%\")\n",
    "    print(f\"  - Non-open source: {(len(exploit_df) - len(open_source_exploits))/len(exploit_df)*100:.1f}%\")\n",
    "    \n",
    "    # Additional stats\n",
    "    if len(exploits_with_cve) > 0:\n",
    "        print(f\"\\nCVE statistics:\")\n",
    "        print(f\"  - Exploits with CVE: {len(exploits_with_cve)} ({len(exploits_with_cve)/len(exploit_df)*100:.1f}%)\")\n",
    "        if osv_cve_set:\n",
    "            cves_in_osv = len(exploits_with_cve[exploits_with_cve['extracted_cves'].apply(\n",
    "                lambda cves: any(cve in osv_cve_set for cve in cves) if cves else False\n",
    "            )])\n",
    "            print(f\"  - Exploits with CVE in OSV: {cves_in_osv} ({cves_in_osv/len(exploits_with_cve)*100:.1f}% of exploits with CVE)\")\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    cols_to_drop = ['extracted_cves', 'has_osv_cve', 'is_open_source']\n",
    "    if 'has_open_source_indicators' in open_source_exploits.columns:\n",
    "        cols_to_drop.append('has_open_source_indicators')\n",
    "    open_source_exploits = open_source_exploits.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Now enrich with OSV data\n",
    "    return enrich_with_osv(open_source_exploits, osv_data_dict)\n",
    "\n",
    "def enrich_with_osv(exploit_df, osv_data_dict):\n",
    "    \"\"\"Enrich ExploitDB data with OSV API information\"\"\"\n",
    "    \n",
    "    if not osv_data_dict:\n",
    "        print(\"No OSV data available for enrichment\")\n",
    "        return exploit_df\n",
    "    \n",
    "    enriched_df = exploit_df.copy()\n",
    "    \n",
    "    # Find CVE column in ExploitDB - check multiple possible columns\n",
    "    exploit_cve_col = None\n",
    "    for col in ['cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID', 'aliases', 'codes', 'code']:\n",
    "        if col in exploit_df.columns:\n",
    "            # Check if this column actually contains CVEs\n",
    "            sample_values = exploit_df[col].dropna().head(10)\n",
    "            has_cve = any(re.search(r'CVE-\\d{4}-\\d{4,}', str(v).upper()) for v in sample_values if pd.notna(v))\n",
    "            if has_cve:\n",
    "                exploit_cve_col = col\n",
    "                print(f\"Using column '{col}' for CVE extraction\")\n",
    "                break\n",
    "    \n",
    "    if exploit_cve_col is None:\n",
    "        print(\"Could not find CVE column in ExploitDB data\")\n",
    "        print(f\"Available columns: {exploit_df.columns.tolist()}\")\n",
    "        # Try to find CVEs in any column\n",
    "        print(\"Attempting to search all columns for CVEs...\")\n",
    "        exploit_cve_col = 'codes'  # Fallback to codes column\n",
    "    \n",
    "    # Extract CVE from each row and enrich with OSV data\n",
    "    def extract_cve_from_value(value):\n",
    "        \"\"\"Extract CVE ID from a value (handles strings with multiple CVEs)\"\"\"\n",
    "        if pd.isna(value) or value == '':\n",
    "            return None\n",
    "        value_str = str(value).upper()\n",
    "        # Find CVE pattern\n",
    "        cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "        matches = re.findall(cve_pattern, value_str)\n",
    "        return matches[0] if matches else None\n",
    "    \n",
    "    # Create columns for OSV data\n",
    "    enriched_df['osv_id'] = None\n",
    "    enriched_df['osv_summary'] = None\n",
    "    enriched_df['osv_severity'] = None\n",
    "    enriched_df['osv_database_specific'] = None\n",
    "    enriched_df['osv_affected'] = None\n",
    "    enriched_df['osv_references'] = None\n",
    "    \n",
    "    matches_found = 0\n",
    "    \n",
    "    for idx, row in enriched_df.iterrows():\n",
    "        cve_value = row[exploit_cve_col]\n",
    "        cve_id = extract_cve_from_value(cve_value)\n",
    "        \n",
    "        if cve_id and cve_id in osv_data_dict:\n",
    "            osv_vuln = osv_data_dict[cve_id]\n",
    "            matches_found += 1\n",
    "            \n",
    "            # Extract relevant OSV information\n",
    "            enriched_df.at[idx, 'osv_id'] = osv_vuln.get('id', None)\n",
    "            enriched_df.at[idx, 'osv_summary'] = osv_vuln.get('summary', None)\n",
    "            \n",
    "            # Extract severity (can be in different places in OSV schema)\n",
    "            severity = None\n",
    "            if 'severity' in osv_vuln and osv_vuln['severity']:\n",
    "                if isinstance(osv_vuln['severity'], list) and len(osv_vuln['severity']) > 0:\n",
    "                    severity = osv_vuln['severity'][0].get('score', None)\n",
    "                elif isinstance(osv_vuln['severity'], dict):\n",
    "                    severity = osv_vuln['severity'].get('score', None)\n",
    "            enriched_df.at[idx, 'osv_severity'] = severity\n",
    "            \n",
    "            # Store database_specific info as JSON string\n",
    "            if 'database_specific' in osv_vuln:\n",
    "                enriched_df.at[idx, 'osv_database_specific'] = json.dumps(osv_vuln['database_specific'])\n",
    "            \n",
    "            # Store affected packages info\n",
    "            if 'affected' in osv_vuln:\n",
    "                enriched_df.at[idx, 'osv_affected'] = json.dumps(osv_vuln['affected'])\n",
    "            \n",
    "            # Store references\n",
    "            if 'references' in osv_vuln:\n",
    "                refs = [ref.get('url', '') for ref in osv_vuln['references']]\n",
    "                enriched_df.at[idx, 'osv_references'] = json.dumps(refs)\n",
    "    \n",
    "    print(f\"\\nEnriched dataset: {len(enriched_df)} exploits\")\n",
    "    print(f\"✓ With OSV match: {matches_found} exploits ({matches_found/len(enriched_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    if matches_found > 0:\n",
    "        osv_with_severity = enriched_df['osv_severity'].notna().sum()\n",
    "        osv_with_summary = enriched_df['osv_summary'].notna().sum()\n",
    "        print(f\"  - With severity: {osv_with_severity}\")\n",
    "        print(f\"  - With summary: {osv_with_summary}\")\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# Filter for open source exploits and enrich with OSV data\n",
    "enriched_df = filter_and_enrich_with_osv(exploitdb_df, osv_data, osv_cve_set)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL OPEN SOURCE EXPLOITS DATASET:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Shape: {enriched_df.shape}\")\n",
    "if len(enriched_df) > 0:\n",
    "    print(f\"OSV columns: {[col for col in enriched_df.columns if col.startswith('osv_')]}\")\n",
    "    enriched_df.head()\n",
    "else:\n",
    "    print(\"No enriched data available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: KEV Splitting\n",
    "\n",
    "Split exploits into KEV vs non-KEV groups using Vulncheck KEV database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (2816031046.py, line 178)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mkev_data = download_kev_data()\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "def download_kev_data():\n",
    "    \"\"\"Download Vulncheck KEV (Known Exploited Vulnerabilities) data\"\"\"\n",
    "    \n",
    "    # Vulncheck API key\n",
    "    VULNCHECK_API_KEY = \"vulncheck_a8192e23c3ca459ecbb8799059d3bbcce69c5b67e8d408cac24206d5c3d48614\"\n",
    "    \n",
    "    # Vulncheck uses Bearer token authentication\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {VULNCHECK_API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    # Try multiple Vulncheck endpoints to find the one with actual CVE data\n",
    "    # Note: The backup endpoint might return file metadata, not the actual data\n",
    "    endpoints = [\n",
    "        \"https://api.vulncheck.com/v3/index/vulncheck-kev\",\n",
    "        \"https://api.vulncheck.com/v3/kev\",\n",
    "        \"https://api.vulncheck.com/v3/kev/list\",\n",
    "        \"https://api.vulncheck.com/v3/index/kev\",\n",
    "        \"https://api.vulncheck.com/v3/backup/vulncheck-kev\",  # Backup endpoint (might need different handling)\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for url in endpoints:\n",
    "        try:\n",
    "            print(f\"\\nTrying endpoint: {url}\")\n",
    "            response = requests.get(url, headers=headers, timeout=60)\n",
    "            \n",
    "            print(f\"  Status code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    kev_data = response.json()\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  ✗ Failed to parse JSON response: {e}\")\n",
    "                    print(f\"    Response text (first 500 chars): {response.text[:500]}\")\n",
    "                    continue\n",
    "                \n",
    "                # Inspect the structure\n",
    "                print(f\"  Response type: {type(kev_data)}\")\n",
    "                if isinstance(kev_data, dict):\n",
    "                    print(f\"  Top-level keys: {list(kev_data.keys())[:10]}\")\n",
    "                    if 'data' in kev_data:\n",
    "                        data_item = kev_data['data']\n",
    "                        print(f\"  'data' type: {type(data_item)}\")\n",
    "                        if isinstance(data_item, list):\n",
    "                            print(f\"  'data' length: {len(data_item)}\")\n",
    "                            if len(data_item) > 0:\n",
    "                                print(f\"  First item type: {type(data_item[0])}\")\n",
    "                                if isinstance(data_item[0], dict):\n",
    "                                    print(f\"  First item keys: {list(data_item[0].keys())[:10]}\")\n",
    "                    elif 'vulnerabilities' in kev_data:\n",
    "                        vuln_item = kev_data['vulnerabilities']\n",
    "                        print(f\"  'vulnerabilities' type: {type(vuln_item)}\")\n",
    "                        if isinstance(vuln_item, list):\n",
    "                            print(f\"  'vulnerabilities' length: {len(vuln_item)}\")\n",
    "                \n",
    "                # Extract CVEs using regex (most reliable method)\n",
    "                all_cves_in_response = set()\n",
    "                try:\n",
    "                    kev_data_str = json.dumps(kev_data)\n",
    "                    cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "                    all_cves_in_response = set(re.findall(cve_pattern, kev_data_str.upper()))\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error extracting CVEs: {e}\")\n",
    "                    all_cves_in_response = set()\n",
    "                \n",
    "                if len(all_cves_in_response) > 0:\n",
    "                    print(f\"\\n✓ SUCCESS: Found {len(all_cves_in_response)} CVEs in Vulncheck KEV!\")\n",
    "                    sample_cves = sorted(list(all_cves_in_response))[:15]\n",
    "                    print(f\"  Sample CVEs: {sample_cves}\")\n",
    "                    \n",
    "                    # Show year distribution\n",
    "                    years = {}\n",
    "                    for cve in all_cves_in_response:\n",
    "                        year = cve.split('-')[1] if len(cve.split('-')) >= 2 else 'unknown'\n",
    "                        years[year] = years.get(year, 0) + 1\n",
    "                    print(f\"\\n  CVE year distribution:\")\n",
    "                    for year in sorted(years.keys()):\n",
    "                        print(f\"    {year}: {years[year]} CVEs\")\n",
    "                    \n",
    "                    # Save raw data\n",
    "                    with open('kev_data.json', 'w') as f:\n",
    "                        json.dump(kev_data, f, indent=2)\n",
    "                    print(f\"\\n  ✓ Saved to kev_data.json\")\n",
    "                    return kev_data\n",
    "                else:\n",
    "                    # Save for inspection\n",
    "                    debug_filename = f'kev_data_debug_{endpoints.index(url)}.json'\n",
    "                    with open(debug_filename, 'w') as f:\n",
    "                        json.dump(kev_data, f, indent=2)\n",
    "                    \n",
    "                    # Try to extract from structure if it's a list of items\n",
    "                    if isinstance(kev_data, dict):\n",
    "                        if 'data' in kev_data:\n",
    "                            if isinstance(kev_data['data'], list) and len(kev_data['data']) > 0:\n",
    "                                first_item = kev_data['data'][0]\n",
    "                                if isinstance(first_item, dict):\n",
    "                                    print(f\"  First item sample: {json.dumps(first_item, indent=2)[:500]}\")\n",
    "                        # Check if it's a backup endpoint response (might have file URLs)\n",
    "                        elif 'url' in kev_data or 'download_url' in kev_data:\n",
    "                            print(f\"  ⚠️  This appears to be a backup metadata response, not actual KEV data\")\n",
    "                            print(f\"  Keys: {list(kev_data.keys())}\")\n",
    "                            # If there's a download URL, we could fetch it, but that's complex\n",
    "                    elif isinstance(kev_data, list) and len(kev_data) > 0:\n",
    "                        print(f\"  Data is a list with {len(kev_data)} items\")\n",
    "                        if isinstance(kev_data[0], dict):\n",
    "                            print(f\"  First item keys: {list(kev_data[0].keys())[:10]}\")\n",
    "                            print(f\"  First item sample: {json.dumps(kev_data[0], indent=2)[:500]}\")\n",
    "            elif response.status_code == 401:\n",
    "                print(f\"  ✗ Authentication failed (401) - check API key\")\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"  ✗ Forbidden (403) - check API permissions\")\n",
    "            else:\n",
    "                print(f\"  ✗ Unexpected status: {response.status_code}\")\n",
    "                print(f\"    Response: {response.text[:200]}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  ✗ Request timed out\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  ✗ Connection error\")\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            print(f\"  ✗ JSON decode error: {e}\")\n",
    "            print(f\"    Response text (first 500 chars): {response.text[:500] if 'response' in locals() else 'N/A'}\")\n",
    "            continue\n",
    "    \n",
    "    # If all Vulncheck endpoints fail, try CISA KEV as fallback\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All Vulncheck endpoints failed - trying CISA KEV as fallback...\")\n",
    "    print(\"=\"*60)\n",
    "    cisa_result = download_cisa_kev()\n",
    "    \n",
    "    if cisa_result is None:\n",
    "        print(\"\\n⚠️  WARNING: Could not download KEV data from any source!\")\n",
    "        print(\"   The analysis will continue but no KEV matching will be possible.\")\n",
    "        return None\n",
    "    \n",
    "    return cisa_result\n",
    "\n",
    "def download_cisa_kev():\n",
    "    \"\"\"Download CISA KEV catalog - this is the most reliable source with CVEs\"\"\"\n",
    "    url = \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            kev_data = response.json()\n",
    "            \n",
    "            with open('kev_data.json', 'w') as f:\n",
    "                json.dump(kev_data, f, indent=2)\n",
    "            \n",
    "            # Extract CVEs using regex (most reliable)\n",
    "            kev_data_str = json.dumps(kev_data)\n",
    "            cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "            all_cves = set(re.findall(cve_pattern, kev_data_str.upper()))\n",
    "            \n",
    "            vuln_count = len(kev_data.get('vulnerabilities', []))\n",
    "            print(f\"✓ CISA KEV data downloaded: {vuln_count} vulnerabilities\")\n",
    "            print(f\"✓ Found {len(all_cves)} unique CVEs in CISA KEV\")\n",
    "            if len(all_cves) > 0:\n",
    "                sample_cves = sorted(list(all_cves))[:15]\n",
    "                print(f\"Sample CVEs: {sample_cves}\")\n",
    "                \n",
    "                # Show year distribution\n",
    "                years = {}\n",
    "                for cve in all_cves:\n",
    "                    year = cve.split('-')[1] if len(cve.split('-')) >= 2 else 'unknown'\n",
    "                    years[year] = years.get(year, 0) + 1\n",
    "                print(f\"\\nCVE year distribution in CISA KEV:\")\n",
    "                for year in sorted(years.keys()):\n",
    "                    print(f\"  {year}: {years[year]} CVEs\")\n",
    "            \n",
    "            return kev_data\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading CISA KEV: {e}\")\n",
    "        return None\n",
    "\n",
    "kev_data = download_kev_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cve_in_kev_via_api(cve_id, api_key):\n",
    "    \"\"\"Check if a specific CVE is in Vulncheck KEV via API\n",
    "    \n",
    "    According to VulnCheck documentation:\n",
    "    https://docs.vulncheck.com/indices/vulncheck-intelligence#vulncheck-kev\n",
    "    The vulncheck-kev index contains exploited vulnerabilities known to VulnCheck\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Try different endpoints based on VulnCheck API documentation\n",
    "    # The vulncheck-kev index endpoint: https://api.vulncheck.com/v3/index/vulncheck-kev\n",
    "    endpoints = [\n",
    "        f\"https://api.vulncheck.com/v3/index/vulncheck-kev?cve={cve_id}\",\n",
    "        f\"https://api.vulncheck.com/v3/index/vulncheck-kev/{cve_id}\",\n",
    "        f\"https://api.vulncheck.com/v3/kev/{cve_id}\",\n",
    "        f\"https://api.vulncheck.com/v3/kev?cve={cve_id}\",\n",
    "    ]\n",
    "    \n",
    "    for url in endpoints:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # Check if we got meaningful data back\n",
    "                if data:\n",
    "                    # If it's a dict with 'data' key, check if data is not empty\n",
    "                    if isinstance(data, dict):\n",
    "                        if 'data' in data:\n",
    "                            # If data is a list and not empty, CVE is in KEV\n",
    "                            if isinstance(data['data'], list) and len(data['data']) > 0:\n",
    "                                return True\n",
    "                            # If data is a dict and not empty, CVE is in KEV\n",
    "                            elif isinstance(data['data'], dict) and len(data['data']) > 0:\n",
    "                                return True\n",
    "                        # If it's a dict with CVE-related keys, it's probably in KEV\n",
    "                        elif 'cve' in data or 'cveID' in data or 'vulnerabilityName' in data:\n",
    "                            return True\n",
    "                    # If it's a list and not empty, CVE is in KEV\n",
    "                    elif isinstance(data, list) and len(data) > 0:\n",
    "                        return True\n",
    "            elif response.status_code == 404:\n",
    "                # CVE not found in KEV\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            # Continue to next endpoint on error\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "def check_cves_in_kev_batch(cve_ids, api_key, batch_size=50):\n",
    "    \"\"\"Check multiple CVEs in KEV via API - processes in batches with progress\"\"\"\n",
    "    VULNCHECK_API_KEY = api_key\n",
    "    \n",
    "    print(f\"This will make {len(cve_ids)} API requests (in batches of {batch_size})...\")\n",
    "    \n",
    "    cves_in_kev = set()\n",
    "    cves_not_in_kev = set()\n",
    "    \n",
    "    # Process in batches to show progress\n",
    "    total_batches = (len(cve_ids) - 1) // batch_size + 1\n",
    "    \n",
    "    for i in range(0, len(cve_ids), batch_size):\n",
    "        batch = cve_ids[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        \n",
    "        batch_in_kev = 0\n",
    "        for cve_id in batch:\n",
    "            try:\n",
    "                if check_cve_in_kev_via_api(cve_id, VULNCHECK_API_KEY):\n",
    "                    cves_in_kev.add(cve_id)\n",
    "                    batch_in_kev += 1\n",
    "                else:\n",
    "                    cves_not_in_kev.add(cve_id)\n",
    "                # Small delay to be respectful to API\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                # If check fails, assume not in KEV\n",
    "                cves_not_in_kev.add(cve_id)\n",
    "        \n",
    "        print(f\"✓ {batch_in_kev}/{len(batch)} found in KEV\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"KEV CHECK RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ CVEs found in KEV: {len(cves_in_kev)} ({len(cves_in_kev)/len(cve_ids)*100:.1f}%)\")\n",
    "    print(f\"✗ CVEs NOT in KEV: {len(cves_not_in_kev)} ({len(cves_not_in_kev)/len(cve_ids)*100:.1f}%)\")\n",
    "    \n",
    "    if len(cves_in_kev) > 0:\n",
    "        sample = sorted(list(cves_in_kev))[:15]\n",
    "    \n",
    "    return cves_in_kev, cves_not_in_kev\n",
    "\n",
    "# Extract all unique CVEs from our enriched exploits\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING ALL CVEs FROM EXPLOITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_cve_from_value(value):\n",
    "    \"\"\"Extract CVE ID from a value\"\"\"\n",
    "    if pd.isna(value) or value == '' or value == 'nan':\n",
    "        return []\n",
    "    value_str = str(value).upper()\n",
    "    cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "    matches = re.findall(cve_pattern, value_str)\n",
    "    return matches\n",
    "\n",
    "# Extract CVEs from all exploits\n",
    "all_exploit_cves_list = []\n",
    "cve_source_col = None\n",
    "\n",
    "for col in ['codes', 'aliases', 'cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID', 'description', 'tags']:\n",
    "    if col in enriched_df.columns:\n",
    "        sample = enriched_df[col].dropna().head(10)\n",
    "        has_cve = any(extract_cve_from_value(v) for v in sample if pd.notna(v))\n",
    "        if has_cve:\n",
    "            cve_source_col = col\n",
    "            print(f\"Using column '{col}' to extract CVEs\")\n",
    "            break\n",
    "\n",
    "if cve_source_col:\n",
    "    for idx, row in enriched_df.iterrows():\n",
    "        value = row[cve_source_col]\n",
    "        cves = extract_cve_from_value(value)\n",
    "        all_exploit_cves_list.extend(cves)\n",
    "\n",
    "# Get unique CVEs\n",
    "unique_cves = list(set([cve.upper().strip() for cve in all_exploit_cves_list if cve and cve.startswith('CVE-')]))\n",
    "\n",
    "if len(unique_cves) > 0:\n",
    "    print(f\"Sample CVEs: {unique_cves[:10]}\")\n",
    "\n",
    "# Now check each CVE against KEV API\n",
    "VULNCHECK_API_KEY = \"vulncheck_a8192e23c3ca459ecbb8799059d3bbcce69c5b67e8d408cac24206d5c3d48614\"\n",
    "\n",
    "if len(unique_cves) > 0:\n",
    "    cves_in_kev_set, cves_not_in_kev_set = check_cves_in_kev_batch(unique_cves, VULNCHECK_API_KEY, batch_size=50)\n",
    "    \n",
    "    # Save results\n",
    "    kev_check_results = {\n",
    "        'cves_in_kev': sorted(list(cves_in_kev_set)),\n",
    "        'cves_not_in_kev': sorted(list(cves_not_in_kev_set))\n",
    "    }\n",
    "    \n",
    "    with open('kev_check_results.json', 'w') as f:\n",
    "        json.dump(kev_check_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to kev_check_results.json\")\n",
    "else:\n",
    "    cves_in_kev_set = set()\n",
    "    cves_not_in_kev_set = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_kev_using_api_results(enriched_df, cves_in_kev_set):\n",
    "    \"\"\"Split exploits based on API check results - much more reliable!\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SPLITTING EXPLOITS BASED ON KEV API CHECK RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract CVEs from each exploit\n",
    "    def extract_cve_from_value(value):\n",
    "        \"\"\"Extract CVE ID from a value\"\"\"\n",
    "        if pd.isna(value) or value == '' or value == 'nan':\n",
    "            return []\n",
    "        value_str = str(value).upper()\n",
    "        cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "        matches = re.findall(cve_pattern, value_str)\n",
    "        return [cve.upper().strip() for cve in matches]\n",
    "    \n",
    "    # Find CVE column\n",
    "    cve_source_col = None\n",
    "    for col in ['codes', 'aliases', 'cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID']:\n",
    "        if col in enriched_df.columns:\n",
    "            sample = enriched_df[col].dropna().head(10)\n",
    "            has_cve = any(extract_cve_from_value(v) for v in sample if pd.notna(v))\n",
    "            if has_cve:\n",
    "                cve_source_col = col\n",
    "                print(f\"Using column '{col}' to extract CVEs from exploits\")\n",
    "                break\n",
    "    \n",
    "    if cve_source_col is None:\n",
    "        print(\"WARNING: Could not find CVE column\")\n",
    "        return pd.DataFrame(), enriched_df.copy()\n",
    "    \n",
    "    # Extract CVEs for each exploit\n",
    "    enriched_df['extracted_cves'] = enriched_df[cve_source_col].apply(extract_cve_from_value)\n",
    "    \n",
    "    # Check each exploit against KEV results\n",
    "    enriched_df['has_kev_cve'] = enriched_df['extracted_cves'].apply(\n",
    "        lambda cves: any(cve in cves_in_kev_set for cve in cves) if cves else False\n",
    "    )\n",
    "    \n",
    "    # Split the dataset\n",
    "    found_mask = enriched_df['has_kev_cve'] == True\n",
    "    not_found_mask = ~found_mask\n",
    "    \n",
    "    found_in_kev = enriched_df[found_mask].copy()\n",
    "    not_found_in_kev = enriched_df[not_found_mask].copy()\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    found_in_kev = found_in_kev.drop(columns=['extracted_cves', 'has_kev_cve'], errors='ignore')\n",
    "    not_found_in_kev = not_found_in_kev.drop(columns=['extracted_cves', 'has_kev_cve'], errors='ignore')\n",
    "    \n",
    "    # Statistics\n",
    "    total_original = len(enriched_df)\n",
    "    total_split = len(found_in_kev) + len(not_found_in_kev)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SPLIT RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Found in KEV: {len(found_in_kev)} exploits ({len(found_in_kev)/total_original*100:.1f}%)\")\n",
    "    print(f\"✓ Not found in KEV: {len(not_found_in_kev)} exploits ({len(not_found_in_kev)/total_original*100:.1f}%)\")\n",
    "    \n",
    "    if total_split == total_original:\n",
    "        print(f\"✓ All exploits successfully split!\")\n",
    "    else:\n",
    "        print(f\"⚠️  Warning: {total_original - total_split} exploits missing!\")\n",
    "    \n",
    "    # Save datasets\n",
    "    found_in_kev.to_csv('exploits_found_in_kev.csv', index=False)\n",
    "    not_found_in_kev.to_csv('exploits_not_found_in_kev.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nDatasets saved:\")\n",
    "    \n",
    "    return found_in_kev, not_found_in_kev\n",
    "\n",
    "def split_by_kev(enriched_df, kev_data):\n",
    "    \"\"\"Split the dataset into 'found in KEV' and 'not found in KEV' - ALL exploits must be in one group\"\"\"\n",
    "    \n",
    "    if kev_data is None:\n",
    "        print(\"KEV data not available - cannot split\")\n",
    "        # Return all exploits as \"not found\" if KEV data unavailable\n",
    "        return pd.DataFrame(), enriched_df.copy()\n",
    "    \n",
    "    \n",
    "    # Extract ALL CVEs from KEV data using regex (most reliable method)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING CVEs FROM KEV DATABASE:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Using regex to extract ALL CVEs from KEV data...\")\n",
    "    \n",
    "    kev_data_str = json.dumps(kev_data) if not isinstance(kev_data, str) else kev_data\n",
    "    cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "    kev_cves_regex = re.findall(cve_pattern, kev_data_str.upper())\n",
    "    kev_cves_regex = [cve.upper().strip() for cve in kev_cves_regex]  # Normalize\n",
    "    \n",
    "    print(f\"✓ Found {len(kev_cves_regex)} CVEs using regex extraction\")\n",
    "    \n",
    "    # If no CVEs found with regex, the data structure is wrong\n",
    "    if len(kev_cves_regex) == 0:\n",
    "        print(f\"Inspecting KEV data structure in detail...\")\n",
    "        if isinstance(kev_data, dict):\n",
    "            print(f\"  Top-level keys: {list(kev_data.keys())}\")\n",
    "            if 'vulnerabilities' in kev_data:\n",
    "                print(f\"  'vulnerabilities' is a {type(kev_data['vulnerabilities'])} with {len(kev_data['vulnerabilities'])} items\")\n",
    "                if len(kev_data['vulnerabilities']) > 0:\n",
    "                    first_vuln = kev_data['vulnerabilities'][0]\n",
    "                    print(f\"  First vulnerability keys: {list(first_vuln.keys())}\")\n",
    "                    print(f\"  First vulnerability sample: {json.dumps(first_vuln, indent=2)[:800]}\")\n",
    "            elif 'data' in kev_data:\n",
    "                print(f\"  'data' is a {type(kev_data['data'])}\")\n",
    "                if isinstance(kev_data['data'], list) and len(kev_data['data']) > 0:\n",
    "                    print(f\"  First data item type: {type(kev_data['data'][0])}\")\n",
    "                    if isinstance(kev_data['data'][0], dict):\n",
    "                        print(f\"  First data item keys: {list(kev_data['data'][0].keys())}\")\n",
    "                        print(f\"  First data item sample: {json.dumps(kev_data['data'][0], indent=2)[:800]}\")\n",
    "    \n",
    "    # Also try structured extraction for additional CVEs\n",
    "    print(f\"\\nAlso trying structured extraction...\")\n",
    "    kev_cves = []\n",
    "    \n",
    "    if isinstance(kev_data, dict):\n",
    "        if 'vulnerabilities' in kev_data:\n",
    "            # CISA format - this is the standard format\n",
    "            print(\"Detected CISA format (vulnerabilities key)\")\n",
    "            print(f\"  Processing {len(kev_data['vulnerabilities'])} vulnerabilities...\")\n",
    "            for v in kev_data['vulnerabilities']:\n",
    "                # CISA uses 'cveID' field\n",
    "                cve = v.get('cveID') or v.get('cve') or v.get('cve_id') or v.get('CVE')\n",
    "                if cve:\n",
    "                    cve_clean = str(cve).upper().strip()\n",
    "                    if cve_clean.startswith('CVE-'):\n",
    "                        kev_cves.append(cve_clean)\n",
    "            print(f\"  Extracted {len(set(kev_cves))} unique CVEs from structured extraction\")\n",
    "        elif 'data' in kev_data:\n",
    "            # Vulncheck format\n",
    "            print(\"Detected Vulncheck format (data key)\")\n",
    "            if isinstance(kev_data['data'], list):\n",
    "                print(f\"  Data is a list with {len(kev_data['data'])} items\")\n",
    "                for v in kev_data['data']:\n",
    "                    if isinstance(v, dict):\n",
    "                        # Try multiple CVE field names\n",
    "                        cve = v.get('cveID') or v.get('cve') or v.get('cve_id') or v.get('cveID') or v.get('id')\n",
    "                        if cve:\n",
    "                            kev_cves.append(str(cve).upper().strip())\n",
    "            elif isinstance(kev_data['data'], dict):\n",
    "                print(f\"  Data is a dict with {len(kev_data['data'])} keys\")\n",
    "                for v in kev_data['data'].values():\n",
    "                    if isinstance(v, dict):\n",
    "                        cve = v.get('cveID') or v.get('cve') or v.get('cve_id') or v.get('id')\n",
    "                        if cve:\n",
    "                            kev_cves.append(str(cve).upper().strip())\n",
    "        elif 'results' in kev_data:\n",
    "            # Alternative format\n",
    "            print(\"Detected results format\")\n",
    "            for v in kev_data['results']:\n",
    "                if isinstance(v, dict):\n",
    "                    cve = v.get('cveID') or v.get('cve') or v.get('cve_id') or v.get('id')\n",
    "                    if cve:\n",
    "                        kev_cves.append(str(cve).upper().strip())\n",
    "        else:\n",
    "            # Try to extract from dict values - also check if keys are CVEs\n",
    "            print(\"Trying to extract from dict structure...\")\n",
    "            for key, value in kev_data.items():\n",
    "                # Check if key itself is a CVE\n",
    "                if isinstance(key, str) and key.upper().startswith('CVE-'):\n",
    "                    kev_cves.append(key.upper().strip())\n",
    "                # Check value\n",
    "                if isinstance(value, dict):\n",
    "                    cve = value.get('cveID') or value.get('cve') or value.get('cve_id') or value.get('id')\n",
    "                    if cve:\n",
    "                        kev_cves.append(str(cve).upper().strip())\n",
    "    elif isinstance(kev_data, list):\n",
    "        print(\"KEV data is a list\")\n",
    "        for v in kev_data:\n",
    "            if isinstance(v, dict):\n",
    "                cve = v.get('cveID') or v.get('cve') or v.get('cve_id') or v.get('id')\n",
    "                if cve:\n",
    "                    kev_cves.append(str(cve).upper().strip())\n",
    "    \n",
    "    # Combine both methods - regex is most reliable\n",
    "    all_kev_cves = list(set(kev_cves + kev_cves_regex))\n",
    "    \n",
    "    # Normalize all CVEs: uppercase, strip whitespace, ensure CVE- format\n",
    "    kev_cves_set = set()\n",
    "    for cve in all_kev_cves:\n",
    "        cve_clean = str(cve).upper().strip()\n",
    "        # Validate CVE format: CVE-YYYY-NNNN+\n",
    "        if cve_clean.startswith('CVE-') and len(cve_clean) > 8:\n",
    "            # Check format: CVE-YYYY-NNNN\n",
    "            parts = cve_clean.split('-')\n",
    "            if len(parts) >= 3 and parts[1].isdigit() and len(parts[1]) == 4:\n",
    "                kev_cves_set.add(cve_clean)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"KEV DATABASE SUMMARY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if len(kev_cves_set) > 0:\n",
    "        sample_kev = sorted(list(kev_cves_set))[:15]\n",
    "        print(f\"Sample KEV CVEs: {sample_kev}\")\n",
    "        \n",
    "        # Show CVE year distribution\n",
    "        years = {}\n",
    "        for cve in kev_cves_set:\n",
    "            year = cve.split('-')[1] if len(cve.split('-')) >= 2 else 'unknown'\n",
    "            years[year] = years.get(year, 0) + 1\n",
    "        print(f\"\\nCVE distribution by year:\")\n",
    "        for year in sorted(years.keys()):\n",
    "            print(f\"  {year}: {years[year]} CVEs\")\n",
    "    else:\n",
    "        print(f\"  Type: {type(kev_data)}\")\n",
    "        if isinstance(kev_data, dict):\n",
    "            print(f\"  Top-level keys: {list(kev_data.keys())[:10]}\")\n",
    "            if 'vulnerabilities' in kev_data:\n",
    "                print(f\"  Vulnerabilities count: {len(kev_data['vulnerabilities'])}\")\n",
    "                if len(kev_data['vulnerabilities']) > 0:\n",
    "                    print(f\"  First vulnerability keys: {list(kev_data['vulnerabilities'][0].keys())[:10]}\")\n",
    "                    print(f\"  First vulnerability sample: {json.dumps(kev_data['vulnerabilities'][0], indent=2)[:500]}\")\n",
    "    \n",
    "    # Extract CVEs from each exploit (same method as OSV enrichment)\n",
    "    def extract_cve_from_value(value):\n",
    "        \"\"\"Extract CVE ID from a value (handles strings with multiple CVEs)\"\"\"\n",
    "        if pd.isna(value) or value == '' or value == 'nan':\n",
    "            return []\n",
    "        value_str = str(value).upper()\n",
    "        # Find CVE pattern\n",
    "        cve_pattern = r'CVE-\\d{4}-\\d{4,}'\n",
    "        matches = re.findall(cve_pattern, value_str)\n",
    "        return matches\n",
    "    \n",
    "    # Find column with CVEs in enriched_df\n",
    "    cve_source_col = None\n",
    "    for col in ['codes', 'aliases', 'cve', 'CVE', 'cve_id', 'CVE_ID', 'cveID']:\n",
    "        if col in enriched_df.columns:\n",
    "            # Check if this column contains CVEs\n",
    "            sample = enriched_df[col].dropna().head(10)\n",
    "            has_cve = any(extract_cve_from_value(v) for v in sample if pd.notna(v))\n",
    "            if has_cve:\n",
    "                cve_source_col = col\n",
    "                print(f\"Using column '{col}' to extract CVEs from exploits\")\n",
    "                break\n",
    "    \n",
    "    if cve_source_col is None:\n",
    "        print(\"WARNING: Could not find CVE column in enriched_df\")\n",
    "        print(f\"Available columns: {enriched_df.columns.tolist()}\")\n",
    "        # If no CVE column, all exploits go to \"not found\"\n",
    "        return pd.DataFrame(), enriched_df.copy()\n",
    "    \n",
    "    # Extract CVEs for each exploit and check against KEV\n",
    "    print(f\"\\nExtracting CVEs from exploits...\")\n",
    "    enriched_df['extracted_cves'] = enriched_df[cve_source_col].apply(extract_cve_from_value)\n",
    "    \n",
    "    # Normalize exploit CVEs: uppercase, strip whitespace\n",
    "    enriched_df['extracted_cves'] = enriched_df['extracted_cves'].apply(\n",
    "        lambda cves: [str(cve).upper().strip() for cve in cves] if cves else []\n",
    "    )\n",
    "    \n",
    "    # Debug: Show some sample CVEs from exploits\n",
    "    all_exploit_cves = set()\n",
    "    for cves in enriched_df['extracted_cves']:\n",
    "        if cves:\n",
    "            all_exploit_cves.update(cves)\n",
    "    \n",
    "    if len(all_exploit_cves) > 0:\n",
    "        sample_exploit = sorted(list(all_exploit_cves))[:10]\n",
    "        print(f\"Sample exploit CVEs: {sample_exploit}\")\n",
    "    \n",
    "    # Find matches between exploit CVEs and KEV CVEs\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MATCHING ANALYSIS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Comparing {len(all_exploit_cves)} exploit CVEs with {len(kev_cves_set)} KEV CVEs...\")\n",
    "    \n",
    "    # Direct intersection (both sets are already normalized to uppercase)\n",
    "    overlap = all_exploit_cves.intersection(kev_cves_set)\n",
    "    \n",
    "    print(f\"\\nCVEs that appear in BOTH exploits and KEV: {len(overlap)}\")\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(f\"✓ SUCCESS: Found {len(overlap)} matching CVEs!\")\n",
    "        sample_matches = sorted(list(overlap))[:15]\n",
    "        print(f\"Sample matching CVEs: {sample_matches}\")\n",
    "        \n",
    "        # Show match statistics\n",
    "        match_percentage = (len(overlap) / len(all_exploit_cves) * 100) if len(all_exploit_cves) > 0 else 0\n",
    "        print(f\"\\nMatch statistics:\")\n",
    "        \n",
    "        matching_cves_set = overlap\n",
    "    else:\n",
    "        print(f\"\\nThis could mean:\")\n",
    "        print(f\"  1. KEV database doesn't contain CVEs from our exploits\")\n",
    "        \n",
    "        # Show samples for comparison\n",
    "        print(f\"\\nSample comparison:\")\n",
    "        if len(kev_cves_set) > 0:\n",
    "            kev_sample = sorted(list(kev_cves_set))[:10]\n",
    "            print(f\"  KEV CVEs sample: {kev_sample}\")\n",
    "        if len(all_exploit_cves) > 0:\n",
    "            exploit_sample = sorted(list(all_exploit_cves))[:10]\n",
    "            print(f\"  Exploit CVEs sample: {exploit_sample}\")\n",
    "        \n",
    "        # Check year overlap\n",
    "        kev_years = {cve.split('-')[1] for cve in kev_cves_set if len(cve.split('-')) >= 2}\n",
    "        exploit_years = {cve.split('-')[1] for cve in all_exploit_cves if len(cve.split('-')) >= 2}\n",
    "        year_overlap = kev_years.intersection(exploit_years)\n",
    "        if year_overlap:\n",
    "            print(f\"  Common CVE years: {sorted(year_overlap)}\")\n",
    "            print(f\"  (This suggests format is OK, but specific CVEs don't match)\")\n",
    "        \n",
    "        # Use KEV set anyway - maybe there's a subtle format difference\n",
    "        matching_cves_set = kev_cves_set\n",
    "        print(f\"\\n⚠️  Will attempt matching with all {len(matching_cves_set)} KEV CVEs\")\n",
    "        print(f\"   (May result in 0 matches if formats truly differ)\")\n",
    "    \n",
    "    # Check each exploit against KEV - be very explicit and robust\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MATCHING EXPLOITS TO KEV:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Matching {len(enriched_df)} exploits against {len(matching_cves_set)} KEV CVEs...\")\n",
    "    \n",
    "    enriched_df['has_kev_cve'] = False\n",
    "    \n",
    "    matches_count = 0\n",
    "    exploits_with_cves = 0\n",
    "    exploits_without_cves = 0\n",
    "    \n",
    "    for idx, row in enriched_df.iterrows():\n",
    "        cves = row['extracted_cves']\n",
    "        if cves:  # If exploit has CVEs\n",
    "            exploits_with_cves += 1\n",
    "            # Check if any CVE from exploit is in KEV\n",
    "            for cve in cves:\n",
    "                # Normalize CVE for comparison\n",
    "                cve_normalized = str(cve).upper().strip()\n",
    "                if cve_normalized in matching_cves_set:\n",
    "                    enriched_df.at[idx, 'has_kev_cve'] = True\n",
    "                    matches_count += 1\n",
    "                    break  # Found a match, no need to check other CVEs\n",
    "        else:\n",
    "            exploits_without_cves += 1\n",
    "    \n",
    "    print(f\"\\nMatching results:\")\n",
    "    print(f\"  - Exploits with CVEs: {exploits_with_cves}\")\n",
    "    print(f\"  - Exploits without CVEs: {exploits_without_cves}\")\n",
    "    print(f\"  - Exploits matched to KEV: {matches_count}\")\n",
    "    print(f\"  - Exploits NOT matched to KEV: {len(enriched_df) - matches_count}\")\n",
    "    \n",
    "    # Split the dataset - EVERY exploit must be in one of the two groups\n",
    "    # Use explicit boolean indexing to ensure all rows are included\n",
    "    found_mask = enriched_df['has_kev_cve'] == True\n",
    "    not_found_mask = ~found_mask  # Everything that is NOT in found\n",
    "    \n",
    "    found_in_kev = enriched_df[found_mask].copy()\n",
    "    not_found_in_kev = enriched_df[not_found_mask].copy()\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    found_in_kev = found_in_kev.drop(columns=['extracted_cves', 'has_kev_cve'], errors='ignore')\n",
    "    not_found_in_kev = not_found_in_kev.drop(columns=['extracted_cves', 'has_kev_cve'], errors='ignore')\n",
    "    \n",
    "    # Verify all exploits are accounted for\n",
    "    total_split = len(found_in_kev) + len(not_found_in_kev)\n",
    "    total_original = len(enriched_df)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL SPLIT RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Found in KEV: {len(found_in_kev)} exploits ({len(found_in_kev)/total_original*100:.1f}%)\")\n",
    "    print(f\"✓ Not found in KEV: {len(not_found_in_kev)} exploits ({len(not_found_in_kev)/total_original*100:.1f}%)\")\n",
    "    \n",
    "    if total_split != total_original:\n",
    "        print(f\"   This should not happen - investigating...\")\n",
    "        # Debug: find missing rows\n",
    "        all_indices = set(enriched_df.index)\n",
    "        found_indices = set(found_in_kev.index)\n",
    "        not_found_indices = set(not_found_in_kev.index)\n",
    "        missing = all_indices - found_indices - not_found_indices\n",
    "        if missing:\n",
    "            print(f\"   Missing indices: {list(missing)[:10]}\")\n",
    "    else:\n",
    "        print(f\"✓ All exploits successfully split into two groups\")\n",
    "    \n",
    "    # Ensure we always return DataFrames (even if empty)\n",
    "    if len(found_in_kev) == 0:\n",
    "        print(f\"   This might mean:\")\n",
    "        print(f\"   1. KEV data doesn't contain matching CVEs\")\n",
    "        print(f\"   2. CVE format mismatch between exploits and KEV\")\n",
    "        print(f\"   3. KEV data structure is different than expected\")\n",
    "    if len(not_found_in_kev) == 0:\n",
    "        print(f\"   This means ALL exploits matched to KEV (unlikely but possible)\")\n",
    "    \n",
    "    # Save both datasets\n",
    "    found_in_kev.to_csv('exploits_found_in_kev.csv', index=False)\n",
    "    not_found_in_kev.to_csv('exploits_not_found_in_kev.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nDatasets saved:\")\n",
    "    \n",
    "    return found_in_kev, not_found_in_kev\n",
    "\n",
    "# Use the new API-based splitting method instead\n",
    "# This checks each CVE individually via API, which is more reliable\n",
    "if 'cves_in_kev_set' in locals() and len(cves_in_kev_set) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USING API-BASED KEV CHECK (MORE RELIABLE)\")\n",
    "    print(\"=\"*60)\n",
    "    found_kev, not_found_kev = split_by_kev_using_api_results(enriched_df, cves_in_kev_set)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FALLING BACK TO OLD METHOD (KEV DATA DOWNLOAD)\")\n",
    "    print(\"=\"*60)\n",
    "    found_kev, not_found_kev = split_by_kev(enriched_df, kev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Similarity Analysis\n",
    "\n",
    "Semantic similarity analysis of exploit code using sentence transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Install AST Parsing Dependencies\n",
    "\n",
    "Install tree-sitter for multi-language AST parsing (Python, Ruby, C, Shell, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tree-sitter for multi-language AST parsing\n",
    "%pip install tree-sitter tree-sitter-languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 AST-based Markov Chains Implementation (Amain Method)\n",
    "\n",
    "Implement the AST-based Markov Chains approach from the paper:\n",
    "- Parse exploit code to AST using tree-sitter\n",
    "- Build Markov chain state transition matrices\n",
    "- Calculate 4 distance metrics (Cosine, Euclidean, Manhattan, Chebyshev)\n",
    "- Train Random Forest classifier for semantic clone detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "# Try to load tree-sitter languages\n",
    "try:\n",
    "    import tree_sitter_python as tspython\n",
    "    PYTHON_LANG_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTHON_LANG_AVAILABLE = False\n",
    "    print(\"⚠️  tree-sitter-python not installed - install with: pip install tree-sitter-python\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Initialize tree-sitter languages\n",
    "# Note: We'll use a simplified approach with unified node types\n",
    "# For production, we'd need to install all language grammars\n",
    "\n",
    "def detect_language(file_path):\n",
    "    \"\"\"Detect programming language from file extension\"\"\"\n",
    "    if pd.isna(file_path) or file_path == '':\n",
    "        return None\n",
    "    \n",
    "    ext = str(file_path).split('.')[-1].lower() if '.' in str(file_path) else None\n",
    "    lang_map = {\n",
    "        'py': 'python',\n",
    "        'c': 'c',\n",
    "        'cpp': 'cpp',\n",
    "        'rb': 'ruby',\n",
    "        'pl': 'perl',\n",
    "        'php': 'php',\n",
    "        'sh': 'bash',\n",
    "        'js': 'javascript',\n",
    "        'java': 'java'\n",
    "    }\n",
    "    return lang_map.get(ext, None)\n",
    "\n",
    "# Unified AST node type dictionary (simplified version of Amain's approach)\n",
    "# We use generic node types that work across languages\n",
    "UNIFIED_NODE_TYPES = {\n",
    "    # Control flow\n",
    "    'function': 0, 'method': 1, 'if': 2, 'else': 3, 'for': 4, 'while': 5,\n",
    "    'return': 6, 'break': 7, 'continue': 8, 'try': 9, 'except': 10, 'catch': 11,\n",
    "    # Expressions\n",
    "    'assignment': 12, 'call': 13, 'identifier': 14, 'literal': 15, 'operator': 16,\n",
    "    'binary': 17, 'unary': 18, 'ternary': 19,\n",
    "    # Data structures\n",
    "    'list': 20, 'dictionary': 21, 'tuple': 22, 'array': 23, 'object': 24,\n",
    "    # Declarations\n",
    "    'variable': 25, 'parameter': 26, 'class': 27, 'module': 28,\n",
    "    # Other\n",
    "    'block': 29, 'statement': 30, 'expression': 31, 'string': 32, 'number': 33,\n",
    "    'comment': 34, 'import': 35, 'require': 36\n",
    "}\n",
    "\n",
    "# Extended to 72 types like Amain (57 AST types + 15 token types)\n",
    "NUM_AST_TYPES = 57\n",
    "NUM_TOKEN_TYPES = 15\n",
    "TOTAL_TYPES = 72\n",
    "\n",
    "def map_node_to_unified_type(node_type, language):\n",
    "    \"\"\"Map language-specific AST node type to unified type\"\"\"\n",
    "    node_type_lower = node_type.lower()\n",
    "    \n",
    "    # Control flow\n",
    "    if 'function' in node_type_lower or 'method' in node_type_lower:\n",
    "        return 'function'\n",
    "    elif 'if' in node_type_lower:\n",
    "        return 'if'\n",
    "    elif 'else' in node_type_lower:\n",
    "        return 'else'\n",
    "    elif 'for' in node_type_lower:\n",
    "        return 'for'\n",
    "    elif 'while' in node_type_lower:\n",
    "        return 'while'\n",
    "    elif 'return' in node_type_lower:\n",
    "        return 'return'\n",
    "    elif 'try' in node_type_lower:\n",
    "        return 'try'\n",
    "    elif 'catch' in node_type_lower or 'except' in node_type_lower:\n",
    "        return 'except'\n",
    "    # Expressions\n",
    "    elif 'call' in node_type_lower or 'invocation' in node_type_lower:\n",
    "        return 'call'\n",
    "    elif 'assignment' in node_type_lower or 'assign' in node_type_lower:\n",
    "        return 'assignment'\n",
    "    elif 'identifier' in node_type_lower:\n",
    "        return 'identifier'\n",
    "    elif 'literal' in node_type_lower or 'constant' in node_type_lower:\n",
    "        return 'literal'\n",
    "    elif 'binary' in node_type_lower:\n",
    "        return 'binary'\n",
    "    elif 'unary' in node_type_lower:\n",
    "        return 'unary'\n",
    "    # Data structures\n",
    "    elif 'list' in node_type_lower or 'array' in node_type_lower:\n",
    "        return 'list'\n",
    "    elif 'dict' in node_type_lower or 'map' in node_type_lower:\n",
    "        return 'dictionary'\n",
    "    # Declarations\n",
    "    elif 'variable' in node_type_lower or 'var' in node_type_lower:\n",
    "        return 'variable'\n",
    "    elif 'parameter' in node_type_lower or 'arg' in node_type_lower:\n",
    "        return 'parameter'\n",
    "    elif 'class' in node_type_lower:\n",
    "        return 'class'\n",
    "    elif 'import' in node_type_lower or 'require' in node_type_lower or 'include' in node_type_lower:\n",
    "        return 'import'\n",
    "    # Defaults\n",
    "    elif 'block' in node_type_lower:\n",
    "        return 'block'\n",
    "    elif 'statement' in node_type_lower:\n",
    "        return 'statement'\n",
    "    elif 'expression' in node_type_lower:\n",
    "        return 'expression'\n",
    "    elif 'string' in node_type_lower:\n",
    "        return 'string'\n",
    "    elif 'number' in node_type_lower or 'int' in node_type_lower or 'float' in node_type_lower:\n",
    "        return 'number'\n",
    "    elif 'comment' in node_type_lower:\n",
    "        return 'comment'\n",
    "    else:\n",
    "        return 'statement'  # Default fallback\n",
    "\n",
    "def parse_code_to_ast(code, language):\n",
    "    \"\"\"Parse code to AST using tree-sitter (simplified - only Python for now)\"\"\"\n",
    "    if not code or code.strip() == '':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if language == 'python' and PYTHON_LANG_AVAILABLE:\n",
    "            parser = Parser()\n",
    "            parser.set_language(Language(tspython.language()))\n",
    "            tree = parser.parse(bytes(code, 'utf8'))\n",
    "            return tree.root_node\n",
    "        # Add other languages as needed\n",
    "        # For now, we'll focus on Python exploits (4584 files)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Silently fail for parsing errors (common with exploit code)\n",
    "        return None\n",
    "\n",
    "def build_markov_matrix_from_ast(ast_node):\n",
    "    \"\"\"\n",
    "    Build Markov chain transition matrix from AST (following Amain approach)\n",
    "    Returns a 57x72 matrix (57 AST node types, 72 possible transitions)\n",
    "    \"\"\"\n",
    "    # Initialize matrix\n",
    "    matrix = np.zeros((NUM_AST_TYPES, TOTAL_TYPES), dtype=np.float64)\n",
    "    \n",
    "    if ast_node is None:\n",
    "        return matrix\n",
    "    \n",
    "    def traverse_ast(node, parent_type=None):\n",
    "        \"\"\"Traverse AST and fill transition matrix\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        # Get unified node type\n",
    "        node_type = map_node_to_unified_type(node.type, 'python')\n",
    "        parent_unified = map_node_to_unified_type(parent_type, 'python') if parent_type else None\n",
    "        \n",
    "        # Map to index (simplified - using modulo for now)\n",
    "        if node_type in UNIFIED_NODE_TYPES:\n",
    "            node_idx = UNIFIED_NODE_TYPES[node_type] % NUM_AST_TYPES\n",
    "        else:\n",
    "            node_idx = hash(node_type) % NUM_AST_TYPES\n",
    "        \n",
    "        # Process children\n",
    "        for child in node.children:\n",
    "            child_type = child.type\n",
    "            child_unified = map_node_to_unified_type(child_type, 'python')\n",
    "            \n",
    "            if child_unified in UNIFIED_NODE_TYPES:\n",
    "                child_idx = UNIFIED_NODE_TYPES[child_unified] % TOTAL_TYPES\n",
    "            else:\n",
    "                child_idx = (hash(child_unified) % NUM_TOKEN_TYPES) + NUM_AST_TYPES\n",
    "            \n",
    "            # Increment transition count\n",
    "            if parent_unified and parent_unified in UNIFIED_NODE_TYPES:\n",
    "                parent_idx = UNIFIED_NODE_TYPES[parent_unified] % NUM_AST_TYPES\n",
    "                matrix[parent_idx, child_idx] += 1\n",
    "            \n",
    "            # Recursively traverse\n",
    "            traverse_ast(child, node_type)\n",
    "    \n",
    "    # Start traversal from root\n",
    "    traverse_ast(ast_node)\n",
    "    \n",
    "    # Normalize rows to get transition probabilities (Markov chain property)\n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "    matrix = matrix / row_sums\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def calculate_distances_between_matrices(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Calculate 4 distance metrics between two Markov chain matrices (Amain approach)\n",
    "    Returns: cosine, euclidean, manhattan, chebyshev distances\n",
    "    \"\"\"\n",
    "    # Flatten matrices for distance calculation\n",
    "    vec1 = matrix1.flatten()\n",
    "    vec2 = matrix2.flatten()\n",
    "    \n",
    "    # Reshape for pairwise distance calculation (Amain uses row-wise distances)\n",
    "    # Following Amain: calculate distances between corresponding rows\n",
    "    cosine_dists = []\n",
    "    euclidean_dists = []\n",
    "    manhattan_dists = []\n",
    "    chebyshev_dists = []\n",
    "    \n",
    "    for i in range(min(matrix1.shape[0], matrix2.shape[0])):\n",
    "        row1 = matrix1[i].reshape(1, -1)\n",
    "        row2 = matrix2[i].reshape(1, -1)\n",
    "        \n",
    "        # Cosine distance (1 - similarity)\n",
    "        cos_sim = cosine_similarity(row1, row2)[0, 0]\n",
    "        cosine_dists.append(1 - cos_sim)\n",
    "        \n",
    "        # Euclidean distance\n",
    "        euc = pairwise_distances(row1, row2, metric='euclidean')[0, 0]\n",
    "        euclidean_dists.append(euc)\n",
    "        \n",
    "        # Manhattan distance\n",
    "        man = pairwise_distances(row1, row2, metric='manhattan')[0, 0]\n",
    "        manhattan_dists.append(man)\n",
    "        \n",
    "        # Chebyshev distance\n",
    "        che = pairwise_distances(row1, row2, metric='chebyshev')[0, 0]\n",
    "        chebyshev_dists.append(che)\n",
    "    \n",
    "    # Pad if matrices have different number of rows\n",
    "    max_rows = max(len(cosine_dists), NUM_AST_TYPES)\n",
    "    while len(cosine_dists) < max_rows:\n",
    "        cosine_dists.append(1.0)  # Maximum distance\n",
    "        euclidean_dists.append(1.0)\n",
    "        manhattan_dists.append(1.0)\n",
    "        chebyshev_dists.append(1.0)\n",
    "    \n",
    "    return cosine_dists[:NUM_AST_TYPES], euclidean_dists[:NUM_AST_TYPES], \\\n",
    "           manhattan_dists[:NUM_AST_TYPES], chebyshev_dists[:NUM_AST_TYPES]\n",
    "\n",
    "def calculate_ast_similarity_amain(found_kev_df, not_found_kev_df, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Calculate AST-based semantic similarity using Amain method (Markov Chains)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"AST-BASED MARKOV CHAINS SIMILARITY ANALYSIS (AMAIN METHOD)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if tree-sitter-python is available\n",
    "    if not PYTHON_LANG_AVAILABLE:\n",
    "        print(\"\\n⚠️  ERROR: tree-sitter-python is not installed!\")\n",
    "        print(\"   Please install it with: pip install tree-sitter-python\")\n",
    "        print(\"   Then restart the kernel and try again.\")\n",
    "        return None\n",
    "    \n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(output_dirs['step4'], 'ast_matrices')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Cache for AST matrices\n",
    "    matrix_cache = {}\n",
    "    \n",
    "    # Cache for exploit code (to avoid re-downloading)\n",
    "    exploit_code_cache = {}\n",
    "    \n",
    "    # Process KEV exploits\n",
    "    print(f\"\\nProcessing {len(found_kev_df)} KEV exploits...\")\n",
    "    kev_matrices = []\n",
    "    kev_indices = []\n",
    "    \n",
    "    python_count = 0\n",
    "    skipped_language = 0\n",
    "    skipped_code = 0\n",
    "    failed_parse = 0\n",
    "    \n",
    "    for idx, row in found_kev_df.iterrows():\n",
    "        file_path = row.get('file')\n",
    "        language = detect_language(file_path)\n",
    "        \n",
    "        if language != 'python':  # Only process Python for now\n",
    "            skipped_language += 1\n",
    "            continue\n",
    "        \n",
    "        python_count += 1\n",
    "        \n",
    "        # Get code (reuse download function with cache)\n",
    "        if file_path in exploit_code_cache:\n",
    "            code = exploit_code_cache[file_path]\n",
    "        else:\n",
    "            code = download_exploit_code(file_path)\n",
    "            if code:\n",
    "                exploit_code_cache[file_path] = code\n",
    "        \n",
    "        if code is None or code.strip() == '':\n",
    "            skipped_code += 1\n",
    "            continue\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{idx}_{language}\"\n",
    "        cache_file = os.path.join(cache_dir, f\"{cache_key}.npy\")\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            matrix = np.load(cache_file)\n",
    "        else:\n",
    "            # Parse to AST\n",
    "            ast_node = parse_code_to_ast(code, language)\n",
    "            if ast_node is None:\n",
    "                failed_parse += 1\n",
    "                continue\n",
    "            \n",
    "            # Build Markov matrix\n",
    "            matrix = build_markov_matrix_from_ast(ast_node)\n",
    "            np.save(cache_file, matrix)\n",
    "        \n",
    "        kev_matrices.append(matrix)\n",
    "        kev_indices.append(idx)\n",
    "        \n",
    "        if len(kev_matrices) % 10 == 0:\n",
    "            print(f\"  Processed {len(kev_matrices)} KEV exploits...\")\n",
    "    \n",
    "    print(f\"✓ Processed {len(kev_matrices)} KEV exploits\")\n",
    "    print(f\"  - Python files detected: {python_count}\")\n",
    "    print(f\"  - Skipped (non-Python): {skipped_language}\")\n",
    "    print(f\"  - Skipped (no code): {skipped_code}\")\n",
    "    print(f\"  - Failed (parse error): {failed_parse}\")\n",
    "    \n",
    "    # Process Non-KEV exploits\n",
    "    print(f\"\\nProcessing {len(not_found_kev_df)} Non-KEV exploits...\")\n",
    "    non_kev_matrices = []\n",
    "    non_kev_indices = []\n",
    "    \n",
    "    python_count_non = 0\n",
    "    skipped_language_non = 0\n",
    "    skipped_code_non = 0\n",
    "    failed_parse_non = 0\n",
    "    \n",
    "    for idx, row in not_found_kev_df.iterrows():\n",
    "        file_path = row.get('file')\n",
    "        language = detect_language(file_path)\n",
    "        \n",
    "        if language != 'python':  # Only process Python for now\n",
    "            skipped_language_non += 1\n",
    "            continue\n",
    "        \n",
    "        python_count_non += 1\n",
    "        \n",
    "        # Get code (reuse download function with cache)\n",
    "        file_path = row.get('file')\n",
    "        if file_path in exploit_code_cache:\n",
    "            code = exploit_code_cache[file_path]\n",
    "        else:\n",
    "            code = download_exploit_code(file_path)\n",
    "            if code:\n",
    "                exploit_code_cache[file_path] = code\n",
    "        \n",
    "        if code is None or code.strip() == '':\n",
    "            skipped_code_non += 1\n",
    "            continue\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{idx}_{language}\"\n",
    "        cache_file = os.path.join(cache_dir, f\"{cache_key}.npy\")\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            matrix = np.load(cache_file)\n",
    "        else:\n",
    "            # Parse to AST\n",
    "            ast_node = parse_code_to_ast(code, language)\n",
    "            if ast_node is None:\n",
    "                failed_parse_non += 1\n",
    "                continue\n",
    "            \n",
    "            # Build Markov matrix\n",
    "            matrix = build_markov_matrix_from_ast(ast_node)\n",
    "            np.save(cache_file, matrix)\n",
    "        \n",
    "        non_kev_matrices.append(matrix)\n",
    "        non_kev_indices.append(idx)\n",
    "        \n",
    "        if len(non_kev_matrices) % 50 == 0:\n",
    "            print(f\"  Processed {len(non_kev_matrices)} Non-KEV exploits...\")\n",
    "    \n",
    "    print(f\"✓ Processed {len(non_kev_matrices)} Non-KEV exploits\")\n",
    "    print(f\"  - Python files detected: {python_count_non}\")\n",
    "    print(f\"  - Skipped (non-Python): {skipped_language_non}\")\n",
    "    print(f\"  - Skipped (no code): {skipped_code_non}\")\n",
    "    print(f\"  - Failed (parse error): {failed_parse_non}\")\n",
    "    \n",
    "    if len(kev_matrices) == 0 or len(non_kev_matrices) == 0:\n",
    "        print(\"⚠️  Not enough AST matrices generated - need Python exploits\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate distances for all pairs\n",
    "    print(f\"\\nCalculating distances for {len(kev_matrices)} x {len(non_kev_matrices)} pairs...\")\n",
    "    all_features = []\n",
    "    all_pairs = []\n",
    "    \n",
    "    for i, kev_matrix in enumerate(kev_matrices):\n",
    "        for j, non_kev_matrix in enumerate(non_kev_matrices):\n",
    "            cosine, euclidean, manhattan, chebyshev = calculate_distances_between_matrices(\n",
    "                kev_matrix, non_kev_matrix\n",
    "            )\n",
    "            \n",
    "            # Feature vector: concatenate all 4 distance metrics (228 features = 57 x 4)\n",
    "            feature_vector = cosine + euclidean + manhattan + chebyshev\n",
    "            all_features.append(feature_vector)\n",
    "            all_pairs.append((kev_indices[i], non_kev_indices[j]))\n",
    "            \n",
    "            if len(all_features) % 1000 == 0:\n",
    "                print(f\"  Calculated {len(all_features)} pairs...\")\n",
    "    \n",
    "    print(f\"✓ Calculated {len(all_features)} feature vectors\")\n",
    "    \n",
    "    # For now, we'll use similarity threshold approach (like Amain paper)\n",
    "    # In production, we'd train a Random Forest classifier\n",
    "    print(\"\\nUsing distance-based similarity (threshold = 0.7)...\")\n",
    "    \n",
    "    # Convert distances to similarities and find top matches\n",
    "    similarity_results = []\n",
    "    threshold = 0.7\n",
    "    \n",
    "    for i, (kev_idx, non_kev_idx) in enumerate(all_pairs):\n",
    "        features = all_features[i]\n",
    "        # Use average cosine distance as similarity score (1 - distance)\n",
    "        avg_cosine_dist = np.mean(features[:NUM_AST_TYPES])\n",
    "        similarity = 1 - avg_cosine_dist\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            similarity_results.append({\n",
    "                'kev_id': int(found_kev_df.loc[kev_idx, 'id']),\n",
    "                'non_kev_id': int(not_found_kev_df.loc[non_kev_idx, 'id']),\n",
    "                'similarity': float(similarity),\n",
    "                'feature_vector': features\n",
    "            })\n",
    "    \n",
    "    print(f\"✓ Found {len(similarity_results)} high-similarity pairs (threshold >= {threshold})\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dirs['step4'], 'ast_markov_similarity_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(similarity_results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to {results_path}\")\n",
    "    \n",
    "    return similarity_results\n",
    "\n",
    "print(\"✓ AST-based Markov Chains functions defined\")\n",
    "print(\"  Note: Currently configured for Python exploits only\")\n",
    "print(\"  To extend to other languages, add tree-sitter language grammars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Run AST-based Similarity Analysis\n",
    "\n",
    "Execute the AST-based Markov Chains analysis on KEV vs Non-KEV exploits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AST-based similarity analysis (Amain method)\n",
    "# First, try to load KEV split data if not already in memory\n",
    "found_kev_df = None\n",
    "not_found_kev_df = None\n",
    "\n",
    "# Check if data is already in memory\n",
    "if 'found_in_kev' in locals() and 'not_found_in_kev' in locals():\n",
    "    found_kev_df = found_in_kev\n",
    "    not_found_kev_df = not_found_in_kev\n",
    "    print(\"✓ Using KEV split data from memory\")\n",
    "elif 'found_in_kev' in globals() and 'not_found_in_kev' in globals():\n",
    "    found_kev_df = found_in_kev\n",
    "    not_found_kev_df = not_found_in_kev\n",
    "    print(\"✓ Using KEV split data from global scope\")\n",
    "else:\n",
    "    # Try to load from CSV files\n",
    "    csv_files = [\n",
    "        ('exploits_found_in_kev.csv', 'found'),\n",
    "        (os.path.join(output_dirs['step3'], 'exploits_found_in_kev.csv'), 'found'),\n",
    "        ('exploits_not_found_in_kev.csv', 'not_found'),\n",
    "        (os.path.join(output_dirs['step3'], 'exploits_not_found_in_kev.csv'), 'not_found')\n",
    "    ]\n",
    "    \n",
    "    for filepath, which in csv_files:\n",
    "        if os.path.exists(filepath):\n",
    "            if which == 'found':\n",
    "                found_kev_df = pd.read_csv(filepath)\n",
    "                print(f\"✓ Loaded KEV exploits from: {filepath} ({len(found_kev_df)} rows)\")\n",
    "            else:\n",
    "                not_found_kev_df = pd.read_csv(filepath)\n",
    "                print(f\"✓ Loaded Non-KEV exploits from: {filepath} ({len(not_found_kev_df)} rows)\")\n",
    "    \n",
    "    if found_kev_df is None or not_found_kev_df is None:\n",
    "        print(\"⚠️  Could not find KEV split data files\")\n",
    "        print(\"   Please run Step 3: KEV Splitting first, or ensure CSV files exist:\")\n",
    "        print(\"   - exploits_found_in_kev.csv\")\n",
    "        print(\"   - exploits_not_found_in_kev.csv\")\n",
    "\n",
    "# Run analysis if we have the data\n",
    "if found_kev_df is not None and not_found_kev_df is not None:\n",
    "    if len(found_kev_df) > 0 and len(not_found_kev_df) > 0:\n",
    "        print(\"\\nRunning AST-based Markov Chains similarity analysis...\")\n",
    "        ast_similarity_results = calculate_ast_similarity_amain(\n",
    "            found_kev_df, \n",
    "            not_found_kev_df,\n",
    "            cache_dir=os.path.join(output_dirs['step4'], 'ast_matrices')\n",
    "        )\n",
    "        \n",
    "        if ast_similarity_results:\n",
    "            print(f\"\\n✓ AST-based analysis complete!\")\n",
    "            print(f\"  Found {len(ast_similarity_results)} high-similarity pairs\")\n",
    "            \n",
    "            # Display top results\n",
    "            top_results = sorted(ast_similarity_results, key=lambda x: x['similarity'], reverse=True)[:10]\n",
    "            print(f\"\\nTop 10 AST-based similarity pairs:\")\n",
    "            print(\"-\" * 80)\n",
    "            for i, result in enumerate(top_results, 1):\n",
    "                print(f\"{i:2d}. KEV ID {result['kev_id']} <-> Non-KEV ID {result['non_kev_id']}: \"\n",
    "                      f\"similarity = {result['similarity']:.4f}\")\n",
    "        else:\n",
    "            print(\"\\n⚠️  AST-based analysis returned no results\")\n",
    "            print(\"   This may be because:\")\n",
    "            print(\"   - No Python exploits found in KEV/Non-KEV groups\")\n",
    "            print(\"   - tree-sitter-python not installed (run: pip install tree-sitter-python)\")\n",
    "            print(\"   - Code parsing failed (exploit code may have syntax errors)\")\n",
    "    else:\n",
    "        print(\"⚠️  One or both datasets are empty\")\n",
    "        print(f\"   KEV exploits: {len(found_kev_df) if found_kev_df is not None else 0}\")\n",
    "        print(f\"   Non-KEV exploits: {len(not_found_kev_df) if not_found_kev_df is not None else 0}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Cannot run AST-based similarity analysis - missing KEV split data\")\n",
    "    print(\"   Please run Step 3: KEV Splitting first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for semantic similarity\n",
    "%pip install sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def download_exploit_code(file_path):\n",
    "    \"\"\"Download exploit code from ExploitDB GitLab repository\"\"\"\n",
    "    if pd.isna(file_path) or file_path == '':\n",
    "        return None\n",
    "    \n",
    "    # ExploitDB GitLab raw URL\n",
    "    base_url = \"https://gitlab.com/exploit-database/exploitdb/-/raw/main/\"\n",
    "    url = base_url + str(file_path)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def prepare_text_for_embedding(row, exploit_code_cache=None):\n",
    "    \"\"\"Extract ONLY the exploit code for embedding (not metadata)\"\"\"\n",
    "    \n",
    "    # Get file path\n",
    "    file_path = row.get('file')\n",
    "    \n",
    "    if pd.isna(file_path) or file_path == '':\n",
    "        return \"No exploit code available\"\n",
    "    \n",
    "    # Check cache first\n",
    "    if exploit_code_cache is not None and file_path in exploit_code_cache:\n",
    "        code = exploit_code_cache[file_path]\n",
    "    else:\n",
    "        # Download exploit code\n",
    "        code = download_exploit_code(file_path)\n",
    "        \n",
    "        # Cache it\n",
    "        if exploit_code_cache is not None:\n",
    "            exploit_code_cache[file_path] = code\n",
    "    \n",
    "    if code is None or code.strip() == '':\n",
    "        return \"No exploit code available\"\n",
    "    \n",
    "    # Return only the code (limit to first 10000 chars to avoid memory issues)\n",
    "    return code[:10000] if len(code) > 10000 else code\n",
    "\n",
    "def calculate_semantic_similarity(found_kev_df, not_found_kev_df, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between exploits in KEV and exploits not in KEV\n",
    "    \n",
    "    Args:\n",
    "        found_kev_df: DataFrame with exploits found in KEV\n",
    "        not_found_kev_df: DataFrame with exploits not found in KEV\n",
    "        model_name: Name of the sentence transformer model to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with similarity results\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SEMANTIC SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    \n",
    "    if len(found_kev_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    if len(not_found_kev_df) == 0:\n",
    "        print(\"⚠️  No exploits not in KEV - cannot perform similarity analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Load the sentence transformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Create cache for exploit code (to avoid re-downloading)\n",
    "    exploit_code_cache = {}\n",
    "    \n",
    "    # Prepare texts for KEV exploits - DOWNLOAD ACTUAL EXPLOIT CODE\n",
    "    print(\"This may take a while as we download the actual exploit code files...\")\n",
    "    kev_texts = []\n",
    "    kev_indices = []\n",
    "    kev_downloaded = 0\n",
    "    kev_failed = 0\n",
    "    \n",
    "    for idx, row in found_kev_df.iterrows():\n",
    "        text = prepare_text_for_embedding(row, exploit_code_cache)\n",
    "        kev_texts.append(text)\n",
    "        kev_indices.append(idx)\n",
    "        \n",
    "        if text != \"No exploit code available\" and text is not None:\n",
    "            kev_downloaded += 1\n",
    "        else:\n",
    "            kev_failed += 1\n",
    "        \n",
    "        if (kev_downloaded + kev_failed) % 10 == 0:\n",
    "    \n",
    "    print(f\"✓ KEV exploits: {kev_downloaded} downloaded, {kev_failed} failed\")\n",
    "    \n",
    "    # Prepare texts for non-KEV exploits - DOWNLOAD ACTUAL EXPLOIT CODE\n",
    "    print(\"This may take a while as we download the actual exploit code files...\")\n",
    "    non_kev_texts = []\n",
    "    non_kev_indices = []\n",
    "    non_kev_downloaded = 0\n",
    "    non_kev_failed = 0\n",
    "    \n",
    "    for idx, row in not_found_kev_df.iterrows():\n",
    "        text = prepare_text_for_embedding(row, exploit_code_cache)\n",
    "        non_kev_texts.append(text)\n",
    "        non_kev_indices.append(idx)\n",
    "        \n",
    "        if text != \"No exploit code available\" and text is not None:\n",
    "            non_kev_downloaded += 1\n",
    "        else:\n",
    "            non_kev_failed += 1\n",
    "        \n",
    "        if (non_kev_downloaded + non_kev_failed) % 50 == 0:\n",
    "    \n",
    "    print(f\"✓ Non-KEV exploits: {non_kev_downloaded} downloaded, {non_kev_failed} failed\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(f\"\\nGenerating embeddings for {len(kev_texts)} KEV exploits...\")\n",
    "    kev_embeddings = model.encode(kev_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(non_kev_texts)} non-KEV exploits...\")\n",
    "    non_kev_embeddings = model.encode(non_kev_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    print(f\"\\nCalculating similarity matrix ({len(kev_embeddings)} x {len(non_kev_embeddings)})...\")\n",
    "    similarity_matrix = cosine_similarity(kev_embeddings, non_kev_embeddings)\n",
    "    \n",
    "    # Find top similar exploits for each KEV exploit\n",
    "    print(\"\\nFinding top similar exploits for each KEV exploit...\")\n",
    "    top_similarities = []\n",
    "    \n",
    "    for i, kev_idx in enumerate(kev_indices):\n",
    "        # Get similarities for this KEV exploit\n",
    "        similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Get top 10 most similar non-KEV exploits\n",
    "        top_indices = np.argsort(similarities)[::-1][:10]\n",
    "        \n",
    "        kev_row = found_kev_df.loc[kev_idx]\n",
    "        kev_cve = str(kev_row.get('codes', 'N/A')) if pd.notna(kev_row.get('codes')) else 'N/A'\n",
    "        kev_desc = str(kev_row.get('description', 'N/A'))[:100] if pd.notna(kev_row.get('description')) else 'N/A'\n",
    "        \n",
    "        top_matches = []\n",
    "        for rank, non_kev_idx_pos in enumerate(top_indices, 1):\n",
    "            non_kev_idx = non_kev_indices[non_kev_idx_pos]\n",
    "            similarity_score = similarities[non_kev_idx_pos]\n",
    "            \n",
    "            non_kev_row = not_found_kev_df.loc[non_kev_idx]\n",
    "            non_kev_cve = str(non_kev_row.get('codes', 'N/A')) if pd.notna(non_kev_row.get('codes')) else 'N/A'\n",
    "            non_kev_desc = str(non_kev_row.get('description', 'N/A'))[:100] if pd.notna(non_kev_row.get('description')) else 'N/A'\n",
    "            \n",
    "            top_matches.append({\n",
    "                'rank': rank,\n",
    "                'similarity_score': float(similarity_score),\n",
    "                'non_kev_id': int(non_kev_row.get('id', 0)) if pd.notna(non_kev_row.get('id')) else 0,\n",
    "                'non_kev_cve': non_kev_cve,\n",
    "                'non_kev_description': non_kev_desc[:200],\n",
    "                'non_kev_platform': str(non_kev_row.get('platform', 'N/A')) if pd.notna(non_kev_row.get('platform')) else 'N/A'\n",
    "            })\n",
    "        \n",
    "        top_similarities.append({\n",
    "            'kev_id': int(kev_row.get('id', 0)) if pd.notna(kev_row.get('id')) else 0,\n",
    "            'kev_cve': kev_cve,\n",
    "            'kev_description': kev_desc[:200],\n",
    "            'kev_platform': str(kev_row.get('platform', 'N/A')) if pd.notna(kev_row.get('platform')) else 'N/A',\n",
    "            'top_similar_exploits': top_matches,\n",
    "            'max_similarity': float(np.max(similarities)),\n",
    "            'avg_similarity': float(np.mean(similarities))\n",
    "        })\n",
    "    \n",
    "    # Statistics\n",
    "    all_max_similarities = [item['max_similarity'] for item in top_similarities]\n",
    "    all_avg_similarities = [item['avg_similarity'] for item in top_similarities]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SIMILARITY STATISTICS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Maximum similarity scores:\")\n",
    "    print(f\"  - Mean: {np.mean(all_max_similarities):.4f}\")\n",
    "    print(f\"  - Median: {np.median(all_max_similarities):.4f}\")\n",
    "    print(f\"  - Min: {np.min(all_max_similarities):.4f}\")\n",
    "    print(f\"  - Max: {np.max(all_max_similarities):.4f}\")\n",
    "    print(f\"\\nAverage similarity scores:\")\n",
    "    print(f\"  - Mean: {np.mean(all_avg_similarities):.4f}\")\n",
    "    print(f\"  - Median: {np.median(all_avg_similarities):.4f}\")\n",
    "    \n",
    "    # Find exploits with high similarity (potential candidates for KEV)\n",
    "    high_similarity_threshold = 0.7\n",
    "    high_similarity_count = sum(1 for s in all_max_similarities if s >= high_similarity_threshold)\n",
    "    print(f\"\\nKEV exploits with similarity >= {high_similarity_threshold}: {high_similarity_count}/{len(top_similarities)}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'similarity_matrix_shape': similarity_matrix.shape,\n",
    "        'statistics': {\n",
    "            'max_similarity_mean': float(np.mean(all_max_similarities)),\n",
    "            'max_similarity_median': float(np.median(all_max_similarities)),\n",
    "            'max_similarity_min': float(np.min(all_max_similarities)),\n",
    "            'max_similarity_max': float(np.max(all_max_similarities)),\n",
    "            'avg_similarity_mean': float(np.mean(all_avg_similarities)),\n",
    "            'avg_similarity_median': float(np.median(all_avg_similarities)),\n",
    "            'high_similarity_count': high_similarity_count\n",
    "        },\n",
    "        'top_similarities': top_similarities\n",
    "    }\n",
    "    \n",
    "    with open('semantic_similarity_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to semantic_similarity_results.json\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform semantic similarity analysis\n",
    "if 'found_kev' in locals() and 'not_found_kev' in locals():\n",
    "    if len(found_kev) > 0 and len(not_found_kev) > 0:\n",
    "        similarity_results = calculate_semantic_similarity(found_kev, not_found_kev)\n",
    "    else:\n",
    "        print(\"⚠️  Cannot perform similarity analysis - one or both groups are empty\")\n",
    "else:\n",
    "    print(\"⚠️  KEV split not yet performed - run the splitting cell first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Metadata Analysis\n",
    "\n",
    "Feature analysis and predictors for high similarity scores.\n",
    "\n",
    "Note: Detailed metadata analysis results are included in the Final Visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualizations\n",
    "\n",
    "Final visualizations for thesis paper.\n",
    "\n",
    "Generate all required visualizations including:\n",
    "- Exploits per year and platform distributions\n",
    "- Similarity distributions and top candidates\n",
    "- KEV vs Non-KEV similarity matrix\n",
    "- CWE types, CVSS correlations, and CISA vs Vulncheck analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Visualizations for Thesis\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "# Load data\n",
    "with open('semantic_similarity_results.json', 'r') as f:\n",
    "    similarity_data = json.load(f)\n",
    "\n",
    "found_kev_df = pd.read_csv('exploits_found_in_kev.csv')\n",
    "not_found_kev_df = pd.read_csv('exploits_not_found_in_kev.csv')\n",
    "\n",
    "# Load similarity results\n",
    "top_similarities = similarity_data.get('top_similarities', [])\n",
    "\n",
    "# Build high similarity pairs (≥0.7)\n",
    "high_sim_pairs = []\n",
    "for kev_item in top_similarities:\n",
    "    kev_id = kev_item.get('kev_id', 0)\n",
    "    kev_cve = kev_item.get('kev_cve', 'N/A')\n",
    "    kev_platform = kev_item.get('kev_platform', 'N/A')\n",
    "    \n",
    "    for match in kev_item.get('top_similar_exploits', []):\n",
    "        similarity = match.get('similarity_score', 0)\n",
    "        if similarity >= 0.7:\n",
    "            high_sim_pairs.append({\n",
    "                'kev_id': kev_id,\n",
    "                'kev_cve': kev_cve,\n",
    "                'kev_platform': kev_platform,\n",
    "                'non_kev_id': match.get('non_kev_id', 0),\n",
    "                'non_kev_cve': match.get('non_kev_cve', 'N/A'),\n",
    "                'non_kev_platform': match.get('non_kev_platform', 'N/A'),\n",
    "                'similarity': similarity\n",
    "            })\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs/step6_visualizations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to extract CVE from codes\n",
    "def extract_cve_from_codes(codes_str):\n",
    "    if pd.isna(codes_str) or not codes_str:\n",
    "        return 'N/A'\n",
    "    cves = re.findall(r'CVE-\\d{4}-\\d+', str(codes_str), re.IGNORECASE)\n",
    "    return cves[0] if cves else 'N/A'\n",
    "\n",
    "# Helper function for scientific style\n",
    "def apply_scientific_style(ax, title=None, xlabel=None, ylabel=None):\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel, fontsize=11)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "def get_bw_colors(n):\n",
    "    colors = ['#000000', '#404040', '#808080', '#A0A0A0', '#C0C0C0', '#606060', '#202020']\n",
    "    return [colors[i % len(colors)] for i in range(n)]\n",
    "\n",
    "print(\"Generating final visualizations...\")\n",
    "\n",
    "# 1. Exploits per Year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "all_exploits = pd.concat([found_kev_df, not_found_kev_df])\n",
    "all_exploits['year'] = pd.to_datetime(all_exploits['date_published'], errors='coerce').dt.year\n",
    "year_counts = all_exploits['year'].value_counts().sort_index()\n",
    "ax.bar(year_counts.index, year_counts.values, color='#404040', edgecolor='black', linewidth=0.8)\n",
    "apply_scientific_style(ax, 'Exploits per Year', 'Year', 'Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '1_exploits_per_year.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Top 10 Platforms\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "platform_counts = all_exploits['platform'].value_counts().head(10)\n",
    "colors = get_bw_colors(len(platform_counts))\n",
    "ax.barh(range(len(platform_counts)), platform_counts.values, color=colors, edgecolor='black', linewidth=0.8)\n",
    "ax.set_yticks(range(len(platform_counts)))\n",
    "ax.set_yticklabels(platform_counts.index)\n",
    "apply_scientific_style(ax, 'Top 10 Platforms', 'Count', 'Platform')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '2_top10_platforms.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Similarity Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "similarities = [p['similarity'] for p in high_sim_pairs]\n",
    "ax.hist(similarities, bins=30, edgecolor='black', alpha=0.7, color='#808080')\n",
    "ax.axvline(x=np.mean(similarities), color='black', linestyle='--', linewidth=1.5, \n",
    "           label=f'Mean: {np.mean(similarities):.4f}')\n",
    "apply_scientific_style(ax, 'Similarity Distribution (High Similarity Pairs ≥0.7)', 'Similarity Score', 'Frequency')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '3_similarity_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Top 100 Similarity Scores\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "all_similarities = []\n",
    "for kev_item in top_similarities:\n",
    "    for match in kev_item.get('top_similar_exploits', []):\n",
    "        all_similarities.append(match.get('similarity_score', 0))\n",
    "all_similarities = sorted(all_similarities, reverse=True)[:100]\n",
    "ax.plot(range(1, len(all_similarities) + 1), all_similarities, 'o-', color='#404040', \n",
    "       markersize=4, linewidth=1, markeredgecolor='black', markeredgewidth=0.5)\n",
    "apply_scientific_style(ax, 'Top 100 Similarity Scores', 'Rank', 'Similarity Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '4_top100_similarity_scores.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Top 30 Non-KEV Candidates by Max Similarity\n",
    "non_kev_max_sim = {}\n",
    "for pair in high_sim_pairs:\n",
    "    nid = pair['non_kev_id']\n",
    "    if nid not in non_kev_max_sim or pair['similarity'] > non_kev_max_sim[nid]['max_sim']:\n",
    "        non_kev_max_sim[nid] = {\n",
    "            'max_sim': pair['similarity'],\n",
    "            'cve': pair['non_kev_cve']\n",
    "        }\n",
    "\n",
    "top_30_non_kev = sorted(non_kev_max_sim.items(), key=lambda x: x[1]['max_sim'], reverse=True)[:30]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ids = [str(item[0]) for item in top_30_non_kev]\n",
    "scores = [item[1]['max_sim'] for item in top_30_non_kev]\n",
    "colors = get_bw_colors(len(ids))\n",
    "ax.barh(range(len(ids)), scores, color=colors, edgecolor='black', linewidth=0.8)\n",
    "ax.set_yticks(range(len(ids)))\n",
    "ax.set_yticklabels(ids, fontsize=8)\n",
    "apply_scientific_style(ax, 'Top 30 Non-KEV Candidates by Max Similarity', 'Max Similarity', 'Non-KEV Exploit ID')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '5_top30_non_kev_candidates.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Top Platforms with High Similarity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "platform_counts_high = Counter([p['kev_platform'] for p in high_sim_pairs])\n",
    "top_platforms = dict(platform_counts_high.most_common(10))\n",
    "colors = get_bw_colors(len(top_platforms))\n",
    "ax.bar(range(len(top_platforms)), list(top_platforms.values()), color=colors, edgecolor='black', linewidth=0.8)\n",
    "ax.set_xticks(range(len(top_platforms)))\n",
    "ax.set_xticklabels(list(top_platforms.keys()), rotation=45, ha='right')\n",
    "apply_scientific_style(ax, 'Top Platforms with High Similarity (KEV)', 'Platform', 'Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '6_top_platforms_high_similarity.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. Top Vulnerability Types with High Similarity\n",
    "# Extract vulnerability types from descriptions\n",
    "vuln_types = []\n",
    "for pair in high_sim_pairs:\n",
    "    kev_row = found_kev_df[found_kev_df['id'] == pair['kev_id']]\n",
    "    if len(kev_row) > 0:\n",
    "        desc = str(kev_row.iloc[0].get('description', '')).lower()\n",
    "        if 'sql injection' in desc or 'sqli' in desc:\n",
    "            vuln_types.append('SQL Injection')\n",
    "        elif 'rce' in desc or 'remote code execution' in desc:\n",
    "            vuln_types.append('RCE')\n",
    "        elif 'xss' in desc or 'cross-site' in desc:\n",
    "            vuln_types.append('XSS')\n",
    "        elif 'authentication' in desc or 'auth bypass' in desc:\n",
    "            vuln_types.append('Auth Bypass')\n",
    "        elif 'path traversal' in desc:\n",
    "            vuln_types.append('Path Traversal')\n",
    "        elif 'command injection' in desc:\n",
    "            vuln_types.append('Command Injection')\n",
    "        else:\n",
    "            vuln_types.append('Other')\n",
    "\n",
    "vuln_type_counts = Counter(vuln_types)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_vuln_types = dict(vuln_type_counts.most_common(10))\n",
    "colors = get_bw_colors(len(top_vuln_types))\n",
    "ax.barh(range(len(top_vuln_types)), list(top_vuln_types.values()), color=colors, edgecolor='black', linewidth=0.8)\n",
    "ax.set_yticks(range(len(top_vuln_types)))\n",
    "ax.set_yticklabels(list(top_vuln_types.keys()))\n",
    "apply_scientific_style(ax, 'Top Vulnerability Types with High Similarity', 'Count', 'Vulnerability Type')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '7_top_vulnerability_types.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 8. Platform Match in High Similarity Pairs\n",
    "same_platform = sum(1 for p in high_sim_pairs if p['kev_platform'] == p['non_kev_platform'])\n",
    "diff_platform = len(high_sim_pairs) - same_platform\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "categories = ['Same Platform', 'Different Platform']\n",
    "counts = [same_platform, diff_platform]\n",
    "colors = ['#000000', '#808080']\n",
    "ax.bar(categories, counts, color=colors, edgecolor='black', linewidth=0.8)\n",
    "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "    ax.text(i, count, f'{count}\\n({count/len(high_sim_pairs)*100:.1f}%)', \n",
    "           ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "apply_scientific_style(ax, 'Platform Match in High Similarity Pairs', 'Category', 'Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '8_platform_match.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 9. KEV vs Non-KEV Matrix of High Similarity Pairs\n",
    "# Build comprehensive matrix table with KEV CVE in first column, then all matching Non-KEV CVEs\n",
    "kev_ids_with_matches = sorted(set(p['kev_id'] for p in high_sim_pairs), \n",
    "                              key=lambda x: len([p for p in high_sim_pairs if p['kev_id'] == x]), \n",
    "                              reverse=True)\n",
    "\n",
    "# Build matrix table: First column = KEV CVE, then columns for each matching Non-KEV CVE\n",
    "matrix_data = []\n",
    "max_matches = 0\n",
    "\n",
    "for kev_id in kev_ids_with_matches:\n",
    "    kev_row = found_kev_df[found_kev_df['id'] == kev_id]\n",
    "    kev_cve = 'N/A'\n",
    "    if len(kev_row) > 0:\n",
    "        kev_cve = extract_cve_from_codes(kev_row.iloc[0].get('codes', ''))\n",
    "    \n",
    "    # Get all non-KEV matches for this KEV, sorted by similarity\n",
    "    matches = [p for p in high_sim_pairs if p['kev_id'] == kev_id]\n",
    "    matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    row = {'KEV_CVE': kev_cve}\n",
    "    \n",
    "    # Add all matching Non-KEV CVEs with their similarity scores\n",
    "    for i, match in enumerate(matches, 1):\n",
    "        non_kev_row = not_found_kev_df[not_found_kev_df['id'] == match['non_kev_id']]\n",
    "        if len(non_kev_row) > 0:\n",
    "            non_kev_cve = extract_cve_from_codes(non_kev_row.iloc[0].get('codes', ''))\n",
    "        else:\n",
    "            non_kev_cve = match['non_kev_cve'] if match['non_kev_cve'] != 'N/A' else f\"ID_{match['non_kev_id']}\"\n",
    "        \n",
    "        # Format: CVE (similarity)\n",
    "        row[f'Match_{i}'] = f\"{non_kev_cve} ({match['similarity']:.4f})\"\n",
    "    \n",
    "    max_matches = max(max_matches, len(matches))\n",
    "    matrix_data.append(row)\n",
    "\n",
    "# Create DataFrame with all columns\n",
    "columns = ['KEV_CVE'] + [f'Match_{i}' for i in range(1, max_matches + 1)]\n",
    "matrix_df = pd.DataFrame(matrix_data)\n",
    "matrix_df = matrix_df.reindex(columns=columns, fill_value='')\n",
    "\n",
    "# Save to CSV (Excel-compatible)\n",
    "matrix_df.to_csv(os.path.join(output_dir, '9_kev_non_kev_matrix.csv'), index=False)\n",
    "\n",
    "# Create heatmap visualization\n",
    "fig, ax = plt.subplots(figsize=(16, max(12, len(kev_ids_with_matches) * 0.3)))\n",
    "similarity_matrix = np.zeros((len(kev_ids_with_matches), len(non_kev_ids_with_matches)))\n",
    "kev_id_to_idx = {kid: idx for idx, kid in enumerate(kev_ids_with_matches)}\n",
    "non_kev_id_to_idx = {nid: idx for idx, nid in enumerate(non_kev_ids_with_matches)}\n",
    "\n",
    "for pair in high_sim_pairs:\n",
    "    kev_idx = kev_id_to_idx.get(pair['kev_id'])\n",
    "    non_kev_idx = non_kev_id_to_idx.get(pair['non_kev_id'])\n",
    "    if kev_idx is not None and non_kev_idx is not None:\n",
    "        similarity_matrix[kev_idx, non_kev_idx] = pair['similarity']\n",
    "\n",
    "im = ax.imshow(similarity_matrix, aspect='auto', cmap='Greys', vmin=0.7, vmax=1.0)\n",
    "ax.set_xticks(range(len(non_kev_ids_with_matches)))\n",
    "ax.set_yticks(range(len(kev_ids_with_matches)))\n",
    "ax.set_xticklabels([str(nid) for nid in non_kev_ids_with_matches], fontsize=6, rotation=90)\n",
    "ax.set_yticklabels([str(kid) for kid in kev_ids_with_matches], fontsize=7)\n",
    "cbar = plt.colorbar(im, ax=ax, label='Similarity Score')\n",
    "apply_scientific_style(ax, 'KEV vs Non-KEV High Similarity Matrix (≥0.7)', 'Non-KEV Exploit ID', 'KEV Exploit ID')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '9_kev_non_kev_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 10. Top CWE Types by Max Similarity and Distribution\n",
    "# Load CWE data if available\n",
    "try:\n",
    "    cwe_file = 'outputs/step5_metadata_analysis/cwe_similarity_statistics.csv'\n",
    "    if os.path.exists(cwe_file):\n",
    "        cwe_df = pd.read_csv(cwe_file)\n",
    "        \n",
    "        # By max similarity\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        top_cwe_sim = cwe_df.nlargest(15, 'max_similarity_mean')\n",
    "        colors = get_bw_colors(len(top_cwe_sim))\n",
    "        ax1.barh(range(len(top_cwe_sim)), top_cwe_sim['max_similarity_mean'], \n",
    "                color=colors, edgecolor='black', linewidth=0.8)\n",
    "        ax1.set_yticks(range(len(top_cwe_sim)))\n",
    "        ax1.set_yticklabels(top_cwe_sim['cwe'], fontsize=8)\n",
    "        apply_scientific_style(ax1, 'Top 15 CWE Types by Max Similarity', 'Max Similarity', 'CWE Type')\n",
    "        ax1.invert_yaxis()\n",
    "        \n",
    "        # By distribution\n",
    "        top_cwe_count = cwe_df.nlargest(15, 'count')\n",
    "        colors = get_bw_colors(len(top_cwe_count))\n",
    "        ax2.barh(range(len(top_cwe_count)), top_cwe_count['count'], \n",
    "                color=colors, edgecolor='black', linewidth=0.8)\n",
    "        ax2.set_yticks(range(len(top_cwe_count)))\n",
    "        ax2.set_yticklabels(top_cwe_count['cwe'], fontsize=8)\n",
    "        apply_scientific_style(ax2, 'Top 15 CWE Types by Distribution', 'Count', 'CWE Type')\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, '10_cwe_types.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 11. Max Similarity vs CVSS Scores\n",
    "try:\n",
    "    cvss_file = 'outputs/step5_metadata_analysis/cvss_similarity_data.csv'\n",
    "    if os.path.exists(cvss_file):\n",
    "        cvss_df = pd.read_csv(cvss_file)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.scatter(cvss_df['cvss_score'], cvss_df['max_similarity'], \n",
    "                  alpha=0.6, s=50, color='#404040', edgecolors='black', linewidth=0.5)\n",
    "        z = np.polyfit(cvss_df['cvss_score'], cvss_df['max_similarity'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(cvss_df['cvss_score'], p(cvss_df['cvss_score']), \n",
    "               \"k--\", alpha=0.8, linewidth=1.5)\n",
    "        from scipy.stats import pearsonr\n",
    "        corr, p_val = pearsonr(cvss_df['cvss_score'], cvss_df['max_similarity'])\n",
    "        apply_scientific_style(ax, f'Max Similarity vs CVSS Scores\\n(r={corr:.3f})', \n",
    "                              'CVSS Score', 'Max Similarity')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, '11_max_similarity_vs_cvss.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 12. High Sim vs CISA or Vulncheck (Weaponized or Not)\n",
    "try:\n",
    "    cisa_file = 'outputs/step5_metadata_analysis/cisa_vs_vulncheck_kev_with_matches.csv'\n",
    "    if os.path.exists(cisa_file):\n",
    "        cisa_df = pd.read_csv(cisa_file)\n",
    "        kev_with_high_sim = cisa_df[cisa_df['match_count'] > 0]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        source_counts = kev_with_high_sim['kev_source'].value_counts()\n",
    "        colors = ['#000000', '#808080']\n",
    "        ax.bar(source_counts.index, source_counts.values, color=colors[:len(source_counts)], \n",
    "              edgecolor='black', linewidth=0.8)\n",
    "        for i, (source, count) in enumerate(source_counts.items()):\n",
    "            ax.text(i, count, f'{count}\\n({count/len(kev_with_high_sim)*100:.1f}%)', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        apply_scientific_style(ax, 'High Similarity Matches: CISA KEV vs Vulncheck-Only', \n",
    "                              'KEV Source', 'Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, '12_cisa_vs_vulncheck.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 13. Exploits Found in KEV and Not Found in KEV by Year\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "found_kev_df['year'] = pd.to_datetime(found_kev_df['date_published'], errors='coerce').dt.year\n",
    "not_found_kev_df['year'] = pd.to_datetime(not_found_kev_df['date_published'], errors='coerce').dt.year\n",
    "\n",
    "found_years = found_kev_df['year'].value_counts().sort_index()\n",
    "not_found_years = not_found_kev_df['year'].value_counts().sort_index()\n",
    "\n",
    "all_years = sorted(set(list(found_years.index) + list(not_found_years.index)))\n",
    "x = np.arange(len(all_years))\n",
    "width = 0.35\n",
    "\n",
    "found_counts = [found_years.get(year, 0) for year in all_years]\n",
    "not_found_counts = [not_found_years.get(year, 0) for year in all_years]\n",
    "\n",
    "ax.bar(x - width/2, found_counts, width, label='Found in KEV', color='#000000', edgecolor='black', linewidth=0.8)\n",
    "ax.bar(x + width/2, not_found_counts, width, label='Not Found in KEV', color='#808080', edgecolor='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([int(y) for y in all_years])\n",
    "apply_scientific_style(ax, 'Exploits Found in KEV vs Not Found in KEV by Year', 'Year', 'Count')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, '13_kev_vs_non_kev_by_year.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ All visualizations saved to {output_dir}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
